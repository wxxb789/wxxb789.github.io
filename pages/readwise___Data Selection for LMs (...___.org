:PROPERTIES:
:title: readwise/Data Selection for LMs (...
:END:


* metadata
:PROPERTIES:
:author: [[sangmichaelxie on Twitter]]
:full-title: "Data Selection for LMs (..."
:category: [[tweets]]
:url: https://twitter.com/sangmichaelxie/status/1623397365443960832
:image-url: https://pbs.twimg.com/profile_images/1150835354544562183/IgjjhgnS.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Data selection for LMs (GPT-3, PaLM) is done with heuristics that select data by training a classifier for high-quality text. Can we do better?

Turns out we can boost downstream GLUE acc by 2+% by adapting the classic importance resampling algorithm..

https://t.co/8PQtPFiCol
🧵 

![](https://pbs.twimg.com/media/FoaWcFZacAAlttT.jpg) 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Before diving in - we've uploaded pre-filtered datasets to HuggingFace that were selected from The Pile for high-quality text! 
You can also use the code to select custom language modeling datasets from The Pile. 

Pre-filtered Pile data and code: https://t.co/HWrR9LRnpF 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Data selection typically involves filtering a large source of raw data towards some desired target distribution, whether it's high-quality/formal text (e.g., Wikipedia + books) for general-domain LMs like GPT-3 or domain-specific data for specialized LMs like Codex. 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Because of the large scale of raw text data, existing methods (GPT-3, PaLM, Pile) use heuristics: simple classifier + a noisy threshold to select data similar to the target distribution.

But the selected data may lack diversity/be too diverse compared to the desired target dist. 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Our method, Data Selection with Importance Resampling (DSIR), uses importance resampling to select data that *matches* a target dist.

To make importance weight estimation tractable in the high-dimensional space of text, we work in a reduced feature space. 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Which feature space? N-grams! We find that KL reduction, a cheap data metric that measures how much data selection reduces KL to the target n-gram distribution over random sampling, predicts downstream acc very well (r=0.89).

Potential for powering new data-centric workflows! 

![](https://pbs.twimg.com/media/FoaaR2baEAAZXEe.jpg) 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
This leads to our instantiation of DSIR, where we train 2 generative bag-of-ngrams models (one each for the raw and target data) to estimate importance weights. The n-grams are hashed onto a fixed number of virtual tokens (hashing trick) for simplicity and tractability. 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
When selecting for formal/high-quality text (Wiki+books) to train general-domain LMs, DSIR selects data that contains qualitatively more formal text than random selection and heuristic filters. 

This results in 2–2.5% higher downstream acc on GLUE than these baselines. 

![](https://pbs.twimg.com/media/FoaXcTXakAEZQJX.png) 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
When selecting for domain-specific data to train specialized LMs, DSIR gets comparable or better results to expert-curated data on average across 8 tasks from 4 diverse domains.

Shows the potential for automatic data selection to replace manual/bespoke processes! 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Does choice of pretraining data matter? We selected pretraining data for 8 downstream tasks and tried all pretrain->downstream pairs. Using the wrong pretraining data causes a 6% average drop in downstream acc, and drops acc by 30% in one case!

Choice of data matters a lot. 

![](https://pbs.twimg.com/media/FoaYfknaUAUOhX_.png) 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Generally, transfer across domains is very asymmetric. 

Pretraining on data selected for a “CS academic papers” target distribution usually has positive transfer to other domains, while data selected for a “customer reviews” target distribution results in negative transfer... 

![](https://pbs.twimg.com/media/FoaYvLbaUAArqzG.jpg) 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
N-gram features are great for importance resampling: cheap to compute, scalable, and overlap in n-gram distributions matters for downstream transfer.

But other pretraining data factors could matter for downstream acc. Designing the right features could improve data selection. 
#+END_QUOTE\
** 📌 [[2023-02-09]]
#+BEGIN_QUOTE
Joint work with my wonderful collaborators and advisors @ShibaniSan @tengyuma @percyliang! 
#+END_QUOTE\