:PROPERTIES:
:title: readwise/如果对 Sora 有兴趣的话，这个 AGI 之家...
:END:


* metadata
:PROPERTIES:
:author: [[dotey on Twitter]]
:full-title: "如果对 Sora 有兴趣的话，这个 AGI 之家..."
:category: [[tweets]]
:url: https://twitter.com/dotey/status/1777087058068001152
:image-url: https://pbs.twimg.com/profile_images/561086911561736192/6_g58vEs.jpeg
:END:

* Highlights first synced by [[Readwise]] [[2024-04-08]]
** 📌 [[2024-04-08]]
#+BEGIN_QUOTE
如果对 Sora 有兴趣的话，这个 AGI 之家组织的，由 OpenAI Sora 团队核心成员 Tim Brooks 和 Bill Peebles 带来的《通过模拟一切视频生成将带我们走向通用人工智能，终有一天你可以在视频模型上运行 ChatGPT。》视频，值得一看。

首先是关于 Sora 的功能介绍和未来潜力，这部分其实日常关注 Sora 都不陌生，例如时长高达 1 分钟，包含复杂元素如反射和阴影，能生成多种不同风格的视频，如纸艺世界、科幻场景等。未来 Sora 有望推动内容创作的民主化，让更多人参与到创新内容创作中。

接着是 Sora 背后的技术原理，这部分内容比较专业，主要是借鉴语言模型的成功经验，利用 Transformer 模型的可扩展性，将视频数据切割成时空小块，用于训练 Transformer 模型。Sora 能生成多种纵横比的视频内容，使用扩散模型逐步去噪，生成视频，还可以将一段视频转换为另一种风格 (SD 编辑技术)。

其中特别值得一提的就是 Tim 谈到了经典文章《苦涩的教训》https://t.co/bUztLW9xHq 中的观点："长远来看，那些随着规模增长而性能提升的方法，随着计算能力的增加将最终胜出。" 因为随着时间的推移，我们拥有的算力也在不断增加。如果一种方法能够充分利用这一点，那么它就会变得越来越好。

Sora 目前的局限性主要在于处理复杂的物理互动上，但可以将 Sora 视为视频版的 GPT-1，短期内有望取得巨大进展。未来有望让用户以 VR 等方式与视频互动，实时编辑。需要继续扩大模型规模、利用更多数据来提升性能。

为什么说 Sora 是通向 AGI 的重要一步呢？因为 Sora 能够展现对人类互动、身体接触等的详细理解，最终可建模人类思维。随着模型规模扩大，Sora 有望实现类似语言模型的涌现能力，能够模拟智能体，展现 3D 一致性，持久性。作为世界模拟器，从现实世界到虚拟世界 (如 Minecraft) 都可学习。

OpenAI 内部常说的一句玩笑是："终有一天你可以在视频模型上运行 ChatGPT。"

最后是 Q&A 环节，主要回答了以下问题：

观众 1: 好，关于理解 AI 智能体或让 AI 智能体在场景中相互作用的问题，这部分信息是否已经被明确定义了呢？

Sora 中智能体的信息都是隐式表示的，没有明确定义，3D、物体等信息都是隐含学习的，并未明确编码。

观众 1: 你能否谈谈微调的可能性？

艺术家希望在创作过程中能有尽可能多的控制权，如果他们有一个他们非常喜欢并且自己设计的角色，他们肯定希望能够在创作新故事时继续使用这个角色。但 OpenAI 还没有一个明确的路线图，不知道何时可能实现这个目标。

观众 2: 关于语言 Transformer，你是在顺序的方式中进行预测，或者可以说是打补丁。在视觉 Transformer 中，我们按照扫描线的顺序进行，也许我们像蛇一样进行，也就是空间建筑。你是否认为这是视觉 Transformer 的基本限制？如果你这样做，预测的顺序是否影响你对空间建筑的关注？

Sora 实际上在使用扩散模型。因此，它不是像语言模型那样的自我回归 Transformer，我们正在对我们生成的视频进行降噪。我们从一个完全充斥噪声的视频开始，不断迭代运行模型，逐步去除这些噪声。做的次数多了，所有的噪声都被去除，我们就得到了一个样本。事实上，我们没有按照所谓的"扫描线顺序"来操作，因为可以同时在许多空间 - 时间段进行去噪。大多数情况下，我们是对整个视频同时进行去噪处理。在我们的技术报告中，我们也提到了另一种方法，如果需要，可以先生成一个较短的视频，然后再对其进行延长。这也是一种选择，但两种方式都可以使用。可以选择一次性生成整个视频，也可以选择先生成一个较短的视频，再根据需要进行延长。

观众 3: 互联网的创新主要是被形式推动的。你觉得有必要回馈成人行业吗？

并不觉得有这个必要……

观众 4: 你们生成视频的帧率是每秒 30 帧吗？还是说，你们更倾向于在场景中对帧持续时间进行插值处理？我明白，所有核心处理的速度都远不及动画渲染的速度。

Sora 是以 30 帧每秒的速度生成的。

观众 4: 你们尝试过进行汽车碰撞或旋转等动作，来测试图像生成是否能够符合物理模型或基础动作类型吗？

Sora 尝试了几个这样的例子，我认为旋转效果基本还算合理。当然，这并不是完美的。我曾见过 Sora 提供的一些关于汽车照明的样本，我认为它还没能完全掌握牛顿的三个运动定律。

观众 5: 你们现在正试图与 Sora 一起解决的问题，有哪些用户反馈吗？

Tim Brooks: 现在我们的主要工作是与外部艺术家进行互动，了解他们如何使用它，以及他们在使用过程中的反馈。另外，我们也在关注一些红队成员对安全问题的反馈。这些就是我们现在主要关注的两种反馈。就像 Bill 提到的，我们从艺术家那里得到的一条非常有价值的反馈是，他们希望具有更多的控制权，例如，艺术家通常需要控制相机，以及相机的路径。在安全问题上，我们要确保，如果我们能够给更多人使用这个工具，那么它一定要是负责任的，安全的。潜在的滥用，假信息等等，都是我们需要考虑的。这些就是红队工作的关注重点。

观众 6: 所以，有没有可能制作出用户可以真正与之互动的视频，比如通过 VR 设备或者其他方式？假设一个视频正在播放，我在中途停止它，改变几件事情，视频会不会……我能否对视频进行部分编辑，加入这些更改？

目前，Sora 的运行速度仍然较慢，主要是从延迟的角度看。我们通常的设置是，这取决于生成的具体参数，例如，持续时间，分辨率。但如果你要制作这个东西，至少需要几分钟。因此，我们离你描述的那种体验还有一段距离，但我认为这会很酷。

观众 7: 你在构建这个第一版时有什么明确的目标，你在过程中遇到了哪些问题，从中学到了什么经验？

最主要的目标始终是要在至少 30 秒内达到 1080p，这是从项目的早期开始。我们觉得视频生成一直被局限在四秒的 GIF 生成中。因此，这真的是整个项目期间团队的重点关注。在这个过程中，我们发现处理视频数据是多么的痛苦。我想这些视频里有很多。因此，会有很多非常详细且枯燥的工程工作需要完成，才能真正使这些系统运作。我想我们在项目开始时就知道，这会需要付出大量的努力。不过，这的确花费了些时间。

我们一直努力保持整个方法足够简单，有时候这确实比说出来要难。但我认为，我们的主要关注点就是尽可能做出最简单的事情，然后在此基础上进行大规模的扩展。

观众 8: 你们是如何评估提示词效果的？

视频的评估确实具有挑战性。我们采用了多种方式。一个是模型的损失值，低损失值与模型的优良性能相关，所以这是有所帮助的。另一个是你可以通过图像指标来评估单帧的质量。我们确实使用了标准的图像指标来评估帧的质量。还有，我们也花了大量的时间生成样本并自己去检查。虽然在这种情况下，你需要对大量样本进行处理，而不是只对单一的提示词进行处理。因为这个过程可能会有噪声。因此，你可能偶然得到一个好样本，然后认为自己可以改进它。这就好比你要把大量不同的提示词和输出进行对比。

观众 9: 你估计我们需要多少训练数据才能实现通用人工智能？你认为我们在互联网上有足够的数据吗？

我认为我们有足够的数据来实现通用人工智能。我也相信人们总能找到新的方法来改进事物。当我们遇到限制时，我们总会找到新的方法来提高结果。所以我觉得，不论我们手头有什么数据，都足以实现通用人工智能。

完整视频文稿：https://t.co/7V01UVUOE5<video controls><source src="https://video.twimg.com/amplify_video/1777082436494544896/pl/oKk9UDUBnpX6wQ-G.m3u8?tag=14&container=cmaf" type="application/x-mpegURL"><source src="https://video.twimg.com/amplify_video/1777082436494544896/vid/avc1/480x270/0EE_O8luTu-Q_hG8.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/amplify_video/1777082436494544896/vid/avc1/640x360/-_DXibczHf88Kqu9.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/amplify_video/1777082436494544896/vid/avc1/1280x720/PeTiQ8M0mJn166NL.mp4?tag=14" type="video/mp4">Your browser does not support the video tag.</video> 
#+END_QUOTE\
** 📌 [[2024-04-08]]
#+BEGIN_QUOTE
回答观众问题是提到的一个细节：“但如果你要制作这样一个视频，至少需要几分钟。”

也就是他们 CTO Mira 说的没错，一个视频几分钟就能生成👍🏻 
#+END_QUOTE\