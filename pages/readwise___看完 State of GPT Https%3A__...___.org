:PROPERTIES:
:title: readwise/看完 State of GPT Https:__...
:END:


* metadata
:PROPERTIES:
:author: [[cosmtrek on Twitter]]
:full-title: "看完 State of GPT Https://..."
:category: [[tweets]]
:url: https://twitter.com/cosmtrek/status/1663719349188972544
:image-url: https://pbs.twimg.com/profile_images/1426678535893110784/F310H9tA.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** 📌 [[2023-05-31]]
#+BEGIN_QUOTE
看完 State Of GPT https://t.co/knrrEPsPDk，整理了一些关键词，供大家深入研究。想到 OpenAI 在 2022 年 11 月 30 日发布了 ChatGPT，到今天正好过去半年😆

1/大模型训练流程

预训练
1. 大数据量收集，几乎是全网规模的数据量，数 T 个tokens
2. Tokenization
3. 预训练大模型发展历程… 
#+END_QUOTE\
** 📌 [[2023-05-31]]
#+BEGIN_QUOTE
2/应用

Chain of thought
Self - consistency
Self - reflection
Tree of thought
AutoGPT
ReAct （Reasoning and Acting）
Toolformer
ChatGPT plugins
Retrieval-Augmented LLMs
Constrained prompting
Parameter Efficient FineTuning (PEFT)
Low-precision inference
LLaMA
Langchain
Vector… 
#+END_QUOTE\
** 📌 [[2023-05-31]]
#+BEGIN_QUOTE
3/关于训练的建议

1. 迭代周期不会很快，并且需要很多的实践
2. 数据处理流程非常关键且复杂，少不了人工成本和时间成本
3. SFT 相对来说实现起来比较容易，只需要在特定的数据上继续语言建模任务
4. RLHF 非常不稳定，很难训练 
#+END_QUOTE\
** 📌 [[2023-05-31]]
#+BEGIN_QUOTE
4/关于应用的建议

1. 使用GPT-4，这仍是迄今为止功能最强大的模型
2. Prompt 尽可能提供任何相关的上下文和信息，包含详细的任务描述、相关信息和说明。尽可能提供任何相关的上下文和信息
3. 多测一些 Prompt，学一些 Prompt Engineering
4. 尝试使用 few-shots… 
#+END_QUOTE\
** 📌 [[2023-05-31]]
#+BEGIN_QUOTE
5/警惕

1. 模型可能有偏见
2. 模型可能产生幻觉信息
3. 模型可能会捏造，产生推理错误
4. 模型的知识是过时的，有知识截止日期
5. 可能会出现即时注入、越狱攻击、数据中毒攻击

最好是在低风险场景下使用 LLM，始终和人工监督结合起来，将它们用作灵感和建议的来源，大模型更适合做… 
#+END_QUOTE\
** 📌 [[2023-05-31]]
#+BEGIN_QUOTE
6/带货

我们在小红书上建了个账号 PaperBox，推荐和解读热门论文，欢迎关注🫡 

![](https://pbs.twimg.com/media/Fxa5jTJaEAEWfsB.jpg) 
#+END_QUOTE\