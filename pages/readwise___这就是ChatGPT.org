:PROPERTIES:
:title: readwise/这就是ChatGPT
:END:

* metadata
:PROPERTIES:
:author: [[斯蒂芬·沃尔弗拉姆]]
:full-title: "这就是ChatGPT"
:category: [[books]]
:tags:[[微信读书]],
:image-url: https://cdn.weread.qq.com/weread/cover/24/cpplatform_4cn8w4tmgzntjobg9ffeny/s_cpplatform_4cn8w4tmgzntjobg9ffeny1703648785.jpg
:END:
* Highlights first synced by [[Readwise]] [[2024-03-12]]
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
不止一位天才先驱以悲剧结束一生：1943年，沃尔特·皮茨(Walter Pitts)在与沃伦·麦卡洛克(Warren McCulloch)共同提出神经网络的数学表示时才20岁，后来因为与导师维纳失和而脱离学术界，最终因饮酒过度于46岁辞世；1958年，30岁的弗兰克·罗森布拉特(Frank Rosenblatt)通过感知机实际实现了神经网络，而1971年，他在43岁生日那天溺水身亡；反向传播的主要提出者大卫·鲁梅尔哈特(David Rumelhart)则正值盛年（50多岁）就罹患了罕见的不治之症，1998年开始逐渐失智，最终在与病魔斗争十多年后离世…… 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
从下页图中可以清晰地看到，GPT-1的论文发表之后，OpenAI这种有意为之的更加简单的Eecoder-Only架构（准确地讲是带自回归的Encoder-Decoder架构）并没有得到太多关注，风头都被谷歌几个月之后发布的BERT（Encoder-Only架构，准确地讲是Encoder-非自回归的Decoder架构）抢去了。随后，出现了一系列 xxBERT类的很有影响的工作。[插图]（大模型进化树，出自Amazon杨靖锋等2023年4月的论文“Harnessing the Power of LLMs in Practice”）今天，BERT论文的引用数已经超过6.8万，比GPT-1论文的不到6000仍然高了一个数量级。两篇论文的技术路线不同，无论是学术界还是工业界，几乎所有人当时都选择了BERT阵营。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
2022年6月，论文“Emergent Abilities of Large Language Models”发布，第一作者是仅从达特茅斯学院本科毕业两年的谷歌研究员Jason Wei（今年2月，他在谷歌的“精英跳槽潮”中去了OpenAI）。他在论文中研究了大模型的涌现能力，这类能力在小模型中不存在，只有模型规模扩大到一定量级才会出现——也就是我们熟悉的“量变会导致质变”。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
1991年，沃尔弗拉姆又返回研究状态，开始“昼伏夜出”，每天深夜埋头做实验、写作长达十年，出版了1000多页的巨著《一种新科学》(A New Kind of Science)。书中的主要观点是：万事皆计算，宇宙中的各种复杂现象，不论是人产生的还是自然中自发的，都可以用一些规则简单地计算和模拟。Amazon上书评的说法可能更好懂：“伽利略曾宣称自然界是用数学的语言书写的，但沃尔弗拉姆认为自然界是用编程语言（而且是非常简单的编程语言）书写的。”而且这些现象或者系统，比如人类大脑的工作和气象系统的演化，在计算方面是等效的，具有相同的复杂度，这称为“计算等价原理”。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
这本书很畅销，因为它的语言很通俗，又有近千幅图片，但是也受到了学术界尤其是物理界人士的很多批评。这些批评主要集中在书中的理论并不是原创的（图灵关于计算复杂性的工作、康威的生命游戏等都与此类似），而且缺乏数学严谨性，因此很多结论很难经得住检验（比如自然选择不是生物复杂性的根本原因，图灵公司出版的图书《量子计算公开课》的作者斯科特·阿伦森也指出沃尔弗拉姆的方法无法解释量子计算中非常核心的贝尔测试的结果）。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
比如，GPT技术路线的一大核心理念，是用最简单的自回归生成架构来解决无监督学习问题，也就是利用无须人特意标注的原始数据，学习其中对世界的映射。自回归生成架构，就是书中讲得非常通俗的“只是一次添加一个词”。这里特别要注意的是，选择这种架构并不是为了做生成任务，而是为了理解或者学习，是为了实现模型的通用能力。在2020年之前甚至之后的几年里，业界很多专业人士想当然地以为GPT是搞生成任务的，所以选择了无视。殊不知GPT-1论文的标题就是“通过生成式预训练改进语言理解”(“Improving Language Understanding by Generative Pre-Training”)。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
再比如，对于没有太多技术背景或者机器学习背景的读者来说，了解人工智能最新动态时可能遇到的直接困难，是听不懂总是出现的基本概念“模型”“参数（在神经网络中就是权重）”是什么意思，而且这些概念很难讲清楚。本书中，大神作者非常贴心地用直观的例子（函数和旋钮）做了解释（参见“什么是模型”一节）。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
作者在讲解中也没有忽视思想性，比如下面的段落很好地介绍了深度学习的意义：“深度学习”在2012年左右的重大突破与如下发现有关：与权重相对较少时相比，在涉及许多权重时，进行最小化（至少近似）可能会更容易。换句话说，有时候用神经网络解决复杂问题比解决简单问题更容易——这似乎有些违反直觉。大致原因在于，当有很多“权重变量”时，高维空间中有“很多不同的方向”可以引导我们到达最小值；而当变量较少时，很容易陷入局部最小值的“山湖”，无法找到“出去的方向”。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
在神经网络的早期发展阶段，人们倾向于认为应该“让神经网络做尽可能少的事”。例如，在将语音转换为文本时，人们认为应该先分析语音的音频，再将其分解为音素，等等。但是后来发现，（至少对于“类人任务”）最好的方法通常是尝试训练神经网络来“解决端到端的问题”，让它自己“发现”必要的中间特征、编码等。 
#+END_QUOTE\
** 📌 [[2023-12-27]]
#+BEGIN_QUOTE
第一篇最后结合作者的计算不可约理论，将对ChatGPT的理解上升到一个高度，与伊尔亚·苏茨克维在多个访谈里强调的“GPT的大思路是通过生成来获取世界模型的压缩表示”异曲同工。 
#+END_QUOTE\
** 📌 [[2024-01-03]]
#+BEGIN_QUOTE
下面这一段落是非常引人深思的：产生“有意义的人类语言”需要什么？过去，我们可能认为人类大脑必不可少。但现在我们知道，ChatGPT的神经网络也可以做得非常出色……我强烈怀疑ChatGPT的成功暗示了一个重要的“科学”事实：有意义的人类语言实际上比我们所知道的更加结构化、更加简单，最终可能以相当简单的规则来描述如何组织这样的语言。 
#+END_QUOTE\
** 📌 [[2024-01-03]]
#+BEGIN_QUOTE
稍有遗憾的是，本书只重点讲了ChatGPT的预训练部分，而没有过多涉及后面也很重要的几个微调步骤：监督微调(supervised fine-tuning，SFT)、奖励建模和强化学习。这方面比较好的学习资料是OpenAI创始成员、前Tesla AI负责人安德烈·卡帕斯(Andrej Karpathy)2023年5月在微软Build大会上的演讲“State of GPT”。[插图]在本书包含的两篇之外，沃尔弗拉姆还有一篇关于ChatGPT的文章“Will AIs Take All Our Jobs and End Human History—or Not? Well, It's Complicated...”，在更高层次上和更大范围内思考了ChatGPT的意义和影响。它也是《一种新科学》一书的延伸，充分体现了沃尔弗拉姆的思考深度。 
#+END_QUOTE\
** 📌 [[2024-01-03]]
#+BEGIN_QUOTE
根据“计算不可约性原理”（即“总有一些计算是没有捷径来加速或者自动化的”，作者认为这是思考AI未来的核心），复杂系统中总是存在无限的“计算可约区”，这正是人类历史上能不断出现科学创新、发明和发现的空间。所以，人类会不断向前沿进发，而且永远有前沿可以探索。同时，“计算不可约性原理”也决定了，人类、AI、自然界和社会等各种计算系统具有根本的不可预测性，始终存在“收获惊喜的可能”。人类可贵的，是有内在驱动力和内在体验，能够内在地定义目标或者意义，从而最终定义未来。 
#+END_QUOTE\
** 📌 [[2024-01-03]]
#+BEGIN_QUOTE
沃尔弗拉姆经常为好莱坞的科幻电影做技术支持，用Mathematica和Wolfram编程语言生成一些逼真的效果，比较著名的包括《星际穿越》里的黑洞引力透镜效应，以及《降临》里掌握以后就能够超越时空的神奇外星语言，非常富有想象力。 
#+END_QUOTE\
** 📌 [[2024-01-03]]
#+BEGIN_QUOTE
他40年前从纯物理转向复杂系统的研究，就是想解决人类智能等现象的第一性原理，因此有很深的积累。因为他交游广泛，与杰弗里·辛顿、伊尔亚·苏茨克维、达里奥·阿莫迪等关键人物都有交流，所以有第一手资料，保证了技术的准确性。难怪本书出版后，OpenAI的CEO称之为“对ChatGPT原理最佳的解释”。 
#+END_QUOTE\
** 📌 [[2024-01-03]]
#+BEGIN_QUOTE
●最高效的方式是发掘新的可能性，定义对自己有价值的东西。●从现在的回答问题转向学会如何提出问题，以及如何确定哪些问题值得提出。也就是从知识执行转向知识战略。●知识广度和思维清晰度将很重要。●直接学习所有详细的知识已经变得不必要了：我们可以在更高的层次上学习和工作，抽象掉许多具体的细节。“整合”，而不是专业化。尽可能广泛、深入地思考，尽可能多地调用知识和范式。●学会使用工具来做事。过去我们更倚重逻辑和数学，以后要特别注意利用计算范式，并运用与计算直接相关的思维方式。 
#+END_QUOTE\
** 📌 [[2024-01-04]]
#+BEGIN_QUOTE
ChatGPT的基础是人工神经网络（本书中一般简称为神经网络或网络），后者最初是在20世纪40年代为了模拟理想化的大脑运作方式而发明的。 
#+END_QUOTE\
** 📌 [[2024-01-04]]
#+BEGIN_QUOTE
假设我们手里的文本是“The best thing about AI is its ability to”（AI最棒的地方在于它能）。想象一下浏览人类编写的数十亿页文本（比如在互联网上和电子书中），找到该文本的所有实例，然后看看接下来出现的是什么词，以及这些词出现的概率是多少。ChatGPT实际上做了类似的事情，只不过它不是查看字面上的文本，而是寻找在某种程度上“意义匹配”的事物（稍后将解释） 
#+END_QUOTE\
** 📌 [[2024-01-04]]
#+BEGIN_QUOTE
然而，这里出现了一点儿玄学[插图]的意味。 
#+END_QUOTE\
** 📌 [[2024-01-04]]
#+BEGIN_QUOTE
出于某种原因—也许有一天能用科学解释—如果我们总是选择排名最高的词，通常会得到一篇非常“平淡”的文章，完全显示不出任何“创造力”（有时甚至会一字不差地重复前文。但是，如果有时（随机）选择排名较低的词，就会得到一篇“更有趣”的文章。 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
而且，符合玄学思想的是，有一个所谓的“温度”参数来确定低排名词的使用频率。对于文章生成来说，“温度”为0.8似乎最好。（值得强调的是，这里没有使用任何“理论”，“温度”参数只是在实践中被发现有效的一种方法。例如，之所以采用“温度”的概念，是因为碰巧使用了在统计物理学中很常见的某种指数分布[插图]，但它与物理学之间并没有任何实际联系，至少就我们目前所知是这样的。） 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
下图则显示了典型英文文本中字母对［二元（2-gram或bigram）字母］的概率。可能出现的第一个字母横向显示，第二个字母纵向显示。[插图]可以看到，q列中除了u行以外都是空白的（概率为零）。现在不再一次一个字母地生成“词”，而是使用这些二元字母的概率，一次关注两个字母。下面是可以得到的一个结果，其中恰巧包括几个“实际的词”。 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
有了足够多的英文文本，我们不仅可以对单个字母或字母对（二元字母）得到相当好的估计，而且可以对更长的字母串得到不错的估计。如果使用逐渐变长的 n 元(n-gram)字母的概率生成“随机的词”，就能发现它们会显得越来越“真实”。[插图] 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
英语中有大约50000个常用词。通过查看大型的英文语料库（比如几百万本书，总共包含几百亿个词），我们可以估计每个词的常用程度。使用这些信息，就可以开始生成“句子”了，其中的每个词都是独立随机选择的，概率与它们在语料库中出现的概率相同。 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
就像处理字母一样，我们可以不仅考虑单个词的概率，而且考虑词对或更长的 n 元词的概率。以下是考虑词对后得到的5个结果，它们都是从单词cat开始的。 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
在网络爬取结果中可能有几千亿个词，在电子书中可能还有另外几百亿个词。但是，即使只有4万个常用词，可能的二元词的数量也已经达到了16亿，而可能的三元词的数量则达到了60万亿。因此，我们无法根据已有的文本估计所有这些三元词的概率。当涉及包含20个词的“文章片段”时，可能的20元词的数量会大于宇宙中的粒子数量，所以从某种意义上说，永远无法把它们全部写下来。 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
我们能做些什么呢？最佳思路是建立一个模型，让我们能够估计序列出现的概率—即使我们从未在已有的文本语料库中明确看到过这些序列。ChatGPT的核心正是所谓的“大语言模型”，后者已经被构建得能够很好地估计这些概率了。 
#+END_QUOTE\
** 📌 [[2024-01-05]]
#+BEGIN_QUOTE
建立一个模型，用它提供某种计算答案的程序，而不仅仅是在每种情况下测量和记录 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
。你使用的任何模型都有某种特定的基本结构，以及用于拟合数据的一定数量的“旋钮”（也就是可以设置的参数）。ChatGPT使用了许多这样的“旋钮”—实际上有1750亿个。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
用数学术语来说，如果一个神经元有输入[插图]，那么我们要计算[插图]。对于权重 w 和常量 b，通常会为网络中的每个神经元选择不同的值；函数 f 则通常在所有神经元中保持不变。计算[插图]只需要进行矩阵乘法和矩阵加法运算。激活函数 f 则使用了非线性函数（最终会导致非平凡的行为）。下面是一些常用的激活函数，这里使用的是Ramp（或ReLU）。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
更大的神经网络通常能更好地逼近我们所求的函数。在“每个吸引子盆地的中心”，我们通常能确切地得到想要的答案。但在边界处，也就是神经网络“很难下定决心”的地方，情况可能会更加混乱。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
穿着猫咪衣服的狗怎么分？等等。无论输入什么，神经网络都会生成一个答案。结果表明，它的做法相当符合人类的思维方式。正如上面所说的，这并不是我们可以“根据第一性原则推导”出来的事实。这只是一些经验性的发现，至少在某些领域是正确的。但这是神经网络有用的一个关键原因：它们以某种方式捕捉了“类似人类”的做事方式。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
终得到了一个能成功复现我们想要的函数的神经网络。应该如何调整权重呢？基本思想是，在每个阶段看一下我们离想要的函数“有多远”，然后朝更接近该函数的方向更新权重。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
为了明白离目标“有多远”，我们计算“损失函数”（有时也称为“成本函数”）。这里使用了一个简单的(L2)损失函数，就是我们得到的值与真实值之间的差异的平方和。随着训练过程不断进行，我们看到损失函数逐渐减小（遵循特定的“学习曲线”，不同任务的学习曲线不同），直到神经网络成功地复现（或者至少很好地近似）我们想要的函数。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
上图展示了，在仅有两个权重的情况下可能需要进行的最小化工作。但是事实证明，即使有更多的权重（ChatGPT使用了1750亿个权重），也仍然可以进行最小化，至少可以在某种程度上进行近似。实际上，“深度学习”在2012年左右的重大突破与如下发现有关：与权重相对较少时相比，在涉及许多权重时，进行最小化（至少近似）可能会更容易。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
换句话说，有时候用神经网络解决复杂问题比解决简单问题更容易—这似乎有些违反直觉。大致原因在于，当有很多“权重变量”时，高维空间中有“很多不同的方向”可以引导我们到达最小值；而当变量较少时，很容易陷入局部最小值的“山湖”，无法找到“出去的方向”。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
假如在我们给出训练样例的区域之外进行“外插”(extrapolation)，可能会得到截然不同的结果。[插图]哪一个是“正确”的呢？实际上没有办法确定。它们都“与观察到的数据一致”。但它们都对应着“在已知框架外”进行“思考”的不同的“固有方式”。只是有些方式对我们人类来说可能“更合理”。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
在过去的十年中，神经网络训练的艺术已经有了许多进展。是的，它基本上是一门艺术。有时，尤其是回顾过去时，人们在训练中至少可以看到一丝“科学解释”的影子了。但是在大多数情况下，这些解释是通过试错发现的，并且添加了一些想法和技巧，逐渐针对如何使用神经网络建立了一门重要的学问。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
这门学问有几个关键部分。首先是针对特定的任务使用何种神经网络架构的问题。然后是如何获取用于训练神经网络的数据的关键问题。在越来越多的情况下，人们并不从头开始训练网络：一个新的网络可以直接包含另一个已经训练过的网络，或者至少可以使用该网络为自己生成更多的训练样例。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
有人可能会认为，每种特定的任务都需要不同的神经网络架构。但事实上，即使对于看似完全不同的任务，同样的架构通常也能够起作用。在某种程度上，这让人想起了通用计算(universal computation)的概念和我的计算等价性原理(Principle of Computational Equivalence)，但是，正如后面将讨论的那样，我认为这更多地反映了我们通常试图让神经网络去完成的任务是“类人”任务，而神经网络可以捕捉相当普遍的“类人过程”。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
在神经网络的早期发展阶段，人们倾向于认为应该“让神经网络做尽可能少的事”。例如，在将语音转换为文本时，人们认为应该先分析语音的音频，再将其分解为音素，等等。但是后来发现，（至少对于“类人任务”）最好的方法通常是尝试训练神经网络来“解决端到端的问题”，让它自己“发现”必要的中间特征、编码等。还有一种想法是，应该将复杂的独立组件引入神经网络，以便让它有效地“显式实现特定的算法思想”。但结果再次证明，这在大多数情况下并不值得；相反，最好只处理非常简单的组件，并让它们“自我组织”（尽管通常是以我们无法理解的方式）来实现（可能）等效的算法思想。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
但是，如何确定特定的任务需要多大的神经网络呢？这有点像一门艺术。在某种程度上，关键是要知道“任务有多难”。但是类人任务的难度通常很难估计。是的，可能有一种系统化的方法可以通过计算机来非常“机械”地完成任务，但是很难知道是否有一些技巧或捷径有助于更轻松地以“类人水平”完成任务。可能需要枚举一棵巨大的对策树才能“机械”地玩某个游戏，但也可能有一种更简单的（“启发式”）方法来实现“类人的游戏水平”。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
为特定的任务训练神经网络需要多少数据？根据第一性原则很难估计。使用“迁移学习”可以将已经在另一个神经网络中学习到的重要特征列表“迁移过来”，从而显著降低对数据规模的要求。但是，神经网络通常需要“看到很多样例”才能训练好。至少对于某些任务而言，神经网络学问中很重要的一点是，样例的重复可能超乎想象。事实上，不断地向神经网络展示所有的样例是一种标准策略。在每个“训练轮次”（training round或epoch）中，神经网络都会处于至少稍微不同的状态，而且向它“提醒”某个特定的样例对于它“记忆该样例”是有用的。（是的，这或许类似于重复在人类记忆中的有用性。） 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
我们看到的是，如果神经网络太小，它就无法复现我们想要的函数。但是只要超过某个大小，它就没有问题了—前提是至少训练足够长的时间，提供足够的样例。顺便说一句，这些图片说明了神经网络学问中的一点：如果中间有一个“挤压”(squeeze)，迫使一切都通过中间较少的神经元，那么通常可以使用较小的网络。［值得一提的是，“无中间层”（或所谓的“感知机”）网络只能学习基本线性函数，但是只要有一个中间层（至少有足够的神经元），原则上就始终可以任意好地逼近任何函数，尽管为了使其可行地训练，通常会做某种规范化或正则化。］ 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
那么ChatGPT呢？它有一个很好的特点，就是可以进行“无监督学习”，这样更容易获取训练样例。回想一下，ChatGPT的基本任务是弄清楚如何续写一段给定的文本。因此，要获得“训练样例”，要做的就是取一段文本，并将结尾遮盖起来，然后将其用作“训练的输入”，而“输出”则是未被遮盖的完整文本。我们稍后会更详细地讨论这个问题，这里的重点是—（与学习图像内容不同）不需要“明确的标签”，ChatGPT实际上可以直接从它得到的任何文本样例中学习。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
神经网络的实际学习过程是怎样的呢？归根结底，核心在于确定哪些权重能够最好地捕捉给定的训练样例。有各种各样的详细选择和“超参数设置”（之所以这么叫，是因为权重也称为“参数”），可以用来调整如何进行学习。有不同的损失函数可以选择，如平方和、绝对值和，等等。有不同的损失最小化方法，如每一步在权重空间中移动多长的距离，等等。然后还有一些问题，比如“批量”(batch)展示多少个样例来获得要最小化的损失的连续估计。是的，我们可以（像在Wolfram语言中所做的一样）应用机器学习来自动化机器学习，并自动设置超参数等。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
损失通常会在一段时间内逐渐减小，但最终会趋于某个恒定值。如果该值足够小，可以认为训练是成功的；否则可能暗示着需要尝试更改网络的架构。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
未来，是否会有更好的方法来训练神经网络或者完成神经网络的任务呢？我认为答案几乎是肯定的。神经网络的基本思想是利用大量简单（本质上相同）的组件来创建一个灵活的“计算结构”，并使其能够逐步通过学习样例得到改进。在当前的神经网络中，基本上是利用微积分的思想（应用于实数）来进行这种逐步的改进。但越来越清楚的是，重点并不是拥有高精度数值，即使使用当前的方法，8位或更少的数也可能已经足够了。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
但即使仅在现有神经网络的框架内，也仍然存在一个关键限制：神经网络的训练目前基本上是顺序进行的，每批样例的影响都会被反向传播以更新权重。事实上，就目前的计算机硬件而言，即使考虑到GPU，神经网络的大部分在训练期间的大部分时间里也是“空闲”的，一次只有一个部分被更新。从某种意义上说，这是因为当前的计算机往往具有独立于CPU（或GPU）的内存。但大脑中的情况可能不同—每个“记忆元素”（即神经元）也是一个潜在的活跃的计算元素。如果我们能够这样设置未来的计算机硬件，就可能会更高效地进行训练。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
非平凡的数学就是一个很好的例子，但实际而言，一般的例子是计算。最终的问题是计算不可约性。有些计算虽然可能需要很多步才能完成，但实际上可以“简化”为相当直接的东西。但计算不可约性的发现意味着这并不总是有效的。对于一些过程（可能像下面的例子一样），无论如何都必须回溯每个计算步骤才能弄清楚发生了什么。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
说到底，可学习性和计算不可约性之间存在根本的矛盾。学习实际上涉及通过利用规律来压缩数据，但计算不可约性意味着最终对可能存在的规律有一个限制。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
换句话说，能力和可训练性之间存在着一个终极权衡：你越想让一个系统“真正利用”其计算能力，它就越会表现出计算不可约性，从而越不容易被训练；而它在本质上越易于训练，就越不能进行复杂的计算。 
#+END_QUOTE\
** 📌 [[2024-01-06]]
#+BEGIN_QUOTE
换句话说，神经网络能够在写文章的任务中获得成功的原因是，写文章实际上是一个“计算深度较浅”的问题，比我们想象的简单。从某种意义上讲，这使我们距离对于人类如何处理类似于写文章的事情（处理语言）“拥有一种理论”更近了一步。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
如果有一个足够大的神经网络，那么你可能能够做到人类可以轻易做到的任何事情。但是你无法捕捉自然界一般而言可以做到的事情，或者我们用自然界塑造的工具可以做到的事情。而正是这些工具的使用，无论是实用性的还是概念性的，近几个世纪以来使我们超越了“纯粹的无辅助的人类思维”的界限，为人类获取了物理宇宙和计算宇宙之外的很多东西 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
例如，我们可以将词嵌入视为试图在一种“意义空间”中布局词，其中“在意义上相近”的词会出现在相近的位置。实际使用的嵌入（例如在ChatGPT中）往往涉及大量数字列表。但如果将其投影到二维平面上，则可以展示嵌入对词的布局方式。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
这里的关键概念是，我们不直接尝试表征“哪个图像接近哪个图像”，而是考虑一个定义良好、可以获取明确的训练数据的任务（这里是数字识别），然后利用如下事实：在完成这个任务时，神经网络隐含地必须做出相当于“接近度决策”的决策。因此，我们不需要明确地谈论“图像的接近度”，而是只谈论图像代表什么数字的具体问题，然后“让神经网络”隐含地确定这对于“图像的接近度”意味着什么。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
我们刚刚谈论了为图像创建特征（并嵌入）的方法，它的基础实际上是通过（根据我们的训练集）确定一些图像是否对应于同一个手写数字来识别它们的相似性。如果我们有一个训练集，可以识别每个图像属于5000种常见物体（如猫、狗、椅子……）中的哪一种，就可以做更多这样的事情。这样，就能以我们对常见物体的识别为“锚点”创建一个图像嵌入，然后根据神经网络的行为“围绕它进行泛化”。关键是，这种行为只要与我们人类感知和解读图像的方式一致，就将最终成为一种“我们认为正确”且在实践中对执行“类人判断”的任务有用的嵌入。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
如何为神经网络设置这个问题呢？最终，我们必须用数来表述一切。一种方法是为英语中约50000个常用词分别分配一个唯一的数。例如，分配给the的可能是914，分配给cat的可能是3542。（这些是GPT-2实际使用的数。）因此，对于“the ___ cat”的问题，我们的输入可能是 {914, 3542}。输出应该是什么样的呢？应该是一个大约包含50000个数的列表，有效地给出了每个可能“填入”的词的概率。为了找到嵌入，我们再次在神经网络“得到结论”之前“拦截”它的“内部”进程，然后获取此时的数字列表，可以认为这是“每个词的表征”。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
［严格来说，ChatGPT并不处理词，而是处理“标记”(token)—这是一种方便的语言单位，既可以是整个词，也可以只是像pre、ing或ized这样的片段。使用标记使ChatGPT更容易处理罕见词、复合词和非英语词，并且会发明新单词（不论结果好坏）。］ 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
实际上，比起用一系列数对词进行表征，我们还可以做得更好—可以对词序列甚至整个文本块进行这样的表征。ChatGPT内部就是这样进行处理的。它会获取到目前为止的所有文本，并生成一个嵌入向量来表示它。然后，它的目标就是找到下一个可能出现的各个词的概率。它会将答案表示为一个数字列表，这些数基本上给出了大约50000个可能出现的词的概率。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
在前面讨论的神经网络中，任何给定层的每个神经元基本上都与上一层的每个神经元相连（起码有一些权重）。但是，如果处理的数据具有特定的已知结构，则这种全连接网络就（可能）大材小用了。因此，以图像处理的早期阶段为例，通常使用所谓的卷积神经网络（convolutional neural net或convnet），其中的神经元被有效地布局在类似于图像像素的网格上，并且仅与在网格上相邻的神经元相连。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
Transformer的思想是，为组成一段文本的标记序列做与此相似的事情。但是，Transformer不是仅仅定义了序列中可以连接的固定区域，而是引入了“注意力”的概念—即更多地“关注”序列的某些部分，而不是其他部分。也许在将来的某一天，可以启动一个通用神经网络并通过训练来完成所有的定制工作。但至少目前来看，在实践中将事物“模块化”似乎是至关重要的—就像Transformer所做的那样，也可能是我们的大脑所做的那样。 
#+END_QUOTE\
** 📌 [[2024-01-07]]
#+BEGIN_QUOTE
它的操作分为三个基本阶段。第一阶段，它获取与目前的文本相对应的标记序列，并找到表示这些标记的一个嵌入（即由数组成的数组）。第二阶段，它以“标准的神经网络的方式”对此嵌入进行操作，值“像涟漪一样依次通过”网络中的各层，从而产生一个新的嵌入（即一个新的数组）。第三阶段，它获取此数组的最后一部分，并据此生成包含约50000个值的数组，这些值就成了各个可能的下一个标记的概率。（没错，使用的标记数量恰好与英语常用词的数量相当，尽管其中只有约3000个标记是完整的词，其余的则是片段。） 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
输入是一个包含 n 个（由整数1到大约50000表示的）标记的向量。每个标记都（通过一个单层神经网络）被转换为一个嵌入向量（在GPT-2中长度为768，在ChatGPT的GPT-3中长度为12288）。同时，还有一条“二级路径”，它接收标记的（整数）位置序列，并根据这些整数创建另一个嵌入向量。最后，将标记值和标记位置的嵌入向量相加，产生嵌入模块的最终嵌入向量序列。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
在每个这样的注意力块中，都有一组“注意力头”（GPT-2有12个，ChatGPT的GPT-3有96个）—每个都独立地在嵌入向量的不同值块上进行操作。（我们不知道为什么最好将嵌入向量分成不同的部分，也不知道不同的部分“意味”着什么。这只是那些“被发现奏效”的事情之一。） 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
注意力头是做什么的呢？它们基本上是一种在标记序列（即目前已经生成的文本）中进行“回顾”的方式，能以一种有用的形式“打包过去的内容”，以便找到下一个标记。在“概率从何而来”一节中，我们介绍了使用二元词的概率来根据上一个词选择下一个词。Transformer中的“注意力”机制所做的是允许“关注”更早的词，因此可能捕捉到（例如）动词可以如何被联系到出现在句子中很多词之前的名词。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
在经过所有这些注意力块后，Transformer的实际效果是什么？本质上，它将标记序列的原始嵌入集合转换为最终集合。ChatGPT的特定工作方式是，选择此集合中的最后一个嵌入，并对其进行“解码”，以生成应该出现的下一个标记的概率列表。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
ChatGPT的原始输入是一个由数组成的数组（到目前为止标记的嵌入向量）。当ChatGPT“运行”以产生新标记时，这些数就会“依次通过”神经网络的各层，而每个神经元都会“做好本职工作”并将结果传递给下一层的神经元。没有循环和“回顾”。一切都是在网络中“向前馈送”的。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
但是在某种意义上，即使在ChatGPT中，仍然存在一个重复使用计算元素的“外部循环”。因为当ChatGPT要生成一个新的标记时，它总是“读取”（即获取为输入）之前的整个标记序列，包括ChatGPT自己先前“写入”的标记。我们可以认为这种设置意味着ChatGPT确实，至少在其最外层，包含一个“反馈循环”，尽管其中的每次迭代都明确显示为它所生成文本中的一个标记。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
让我们回到ChatGPT的核心：神经网络被反复用于生成每个标记。在某种程度上，它非常简单：就是完全相同的人工神经元的一个集合。网络的某些部分仅由（“全连接”的）神经元层组成，其中给定层的每个神经元都与上一层的每个神经元（以某种权重）相连。但是由于特别的Transformer架构，ChatGPT的一些部分具有其他的结构，其中仅连接不同层的特定神经元。（当然，仍然可以说“所有神经元都连接在一起”，但有些连接的权重为零。） 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
但是，有了所有这些数据，要如何训练神经网络呢？基本过程与上面讨论的简单示例非常相似：先提供一批样例，然后调整网络中的权重，以最小化网络在这些样例上的误差（“损失”）。根据误差“反向传播”的主要问题在于，每次执行此操作时，网络中的每个权重通常都至少会发生微小的变化，而且有很多权重需要处理。（实际的“反向传播”通常只比前向传播难一点儿—相差一个很小的常数系数。） 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
当我们运行ChatGPT来生成文本时，基本上每个权重都需要使用一次。因此，如果有 n 个权重，就需要执行约 n 个计算步骤—尽管在实践中，许多计算步骤通常可以在GPU中并行执行。但是，如果需要约 n 个词的训练数据来设置这些权重，那么如上所述，我们可以得出结论：需要约n²个计算步骤来进行网络的训练。这就是为什么使用当前的方法最终需要耗费数十亿美元来进行训练。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
构建ChatGPT的一个关键思想是，在“被动阅读”来自互联网等的内容之后添加一步：让人类积极地与ChatGPT互动，看看它产生了什么，并且在“如何成为一个好的聊天机器人”方面给予实际反馈。但是神经网络是如何利用这些反馈的呢？首先，仅仅让人类对神经网络的结果评分。然后，建立另一个神经网络模型来预测这些评分。现在，这个预测模型可以在原始网络上运行—本质上像损失函数一样—从而使用人类的反馈对原始网络进行“调优”。实践中的结果似乎对系统能否成功产生“类人”输出有很大的影响。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
它确实有些类人：至少在经过所有预训练后，你只需要把东西告诉它一次，它就能“记住”—至少记住足够长的时间来生成一段文本。这里面到底发生了什么事呢？也许“你可能告诉它的一切都已经在里面的某个地方了”，你只是把它引导到了正确的位置。但这似乎不太可能。更可能的是，虽然这些元素已经在里面了，但具体情况是由类似于“这些元素之间的轨迹”所定义的，而你告诉它的就是这条轨迹。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
值得再次指出的是，神经网络在捕捉信息方面不可避免地存在“算法限制”。如果告诉它类似于“从这个到那个”等“浅显”的规则，神经网络很可能能够不错地表示和重现这些规则，并且它“已经掌握”的语言知识将为其提供一个立即可用的模式。但是，如果试图给它实际的“深度”计算规则，涉及许多可能计算不可约的步骤，那么它就行不通了。（请记住，它在每一步都只是在网络中“向前馈送数据”，除非生成新的标记，否则它不会循环。） 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
之前讨论过，在ChatGPT内部，任何文本都可以被有效地表示为一个由数组成的数组，可以将其视为某种“语言特征空间”中一个点的坐标。因此，ChatGPT续写一段文本，就相当于在语言特征空间中追踪一条轨迹。现在我们会问：是什么让这条轨迹与我们认为有意义的文本相对应呢？是否有某种“语义运动定律”定义（或至少限制）了语言特征空间中的点如何在保持“有意义”的同时到处移动？ 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
但也许我们只是关注了“错的变量”（或者错的坐标系），如果关注对的那一个，就会立即看到ChatGPT正在做“像数学物理一样简单”的事情，比如沿测地线前进。 
#+END_QUOTE\
** 📌 [[2024-01-08]]
#+BEGIN_QUOTE
产生“有意义的人类语言”需要什么？过去，我们可能认为人类大脑必不可少。但现在我们知道，ChatGPT的神经网络也可以做得非常出色。这或许就是我们所能达到的极限，没有比这更简单（或更易于人类理解）的方法可以使用了。不过，我强烈怀疑ChatGPT的成功暗示了一个重要的“科学”事实：有意义的人类语言实际上比我们所知道的更加结构化、更加简单，最终可能以相当简单的规则来描述如何组织这样的语言。 
#+END_QUOTE\
** 📌 [[2024-01-10]]
#+BEGIN_QUOTE
如何确定适用于一般符号话语语言的“本体论”(ontology)呢？这并不容易。也许这就是自亚里士多德2000多年前对本体论做出原始论述以来，在这些方面几乎没有什么进展的原因。但现在，我们已经知道了有关如何以计算的方式来思考世界的许多知识，这确实很有帮助（从我们的Physics Project和ruliad[插图]思想中得到“基本的形而上学”也无妨）。 
#+END_QUOTE\
** 📌 [[2024-01-10]]
#+BEGIN_QUOTE
ChatGPT的基本概念在某种程度上相当简单：首先从互联网、书籍等获取人类创造的海量文本样本，然后训练一个神经网络来生成“与之类似”的文本。特别是，它能够从“提示”开始，继续生成“与其训练数据相似的文本”。 
#+END_QUOTE\
** 📌 [[2024-01-10]]
#+BEGIN_QUOTE
尽管我们才刚刚开始探索这对ChatGPT意味着什么，但很明显，惊喜是可能出现的。虽然Wolfram|Alpha和ChatGPT所做的事情完全不同，做事的方式也完全不同，但它们有一个公共接口：自然语言。这意味着ChatGPT可以像人类一样与Wolfram|Alpha“交谈”，而Wolfram|Alpha会将它从ChatGPT获得的自然语言转换为精确的符号计算语言，从而应用其计算知识能力。 
#+END_QUOTE\
** 📌 [[2024-01-10]]
#+BEGIN_QUOTE
几十年来，对AI的思考一直存在着两极分化：ChatGPT使用的“统计方法”，以及实际上是Wolfram|Alpha的起点的“符号方法”。现在，由于有了ChatGPT的成功以及我们在使Wolfram|Alpha理解自然语言方面所做的所有工作，终于有机会将二者结合起来，发挥出比单独使用任何一种方法都更强大的力量。 
#+END_QUOTE\
** 📌 [[2024-01-11]]
#+BEGIN_QUOTE
在许多方面，可以说ChatGPT从未“真正理解”过事物，它只“知道如何产生有用的东西”。但是Wolfram|Alpha则完全不同。因为一旦Wolfram|Alpha将某些东西转换为Wolfram语言，我们就拥有了它们完整、精确、形式化的表示，可以用来可靠地计算事物。不用说，有很多“人类感兴趣”的事物并没有形式化的计算表示—尽管我们仍然可以用自然语言谈论它们，但是可能不够准确。对于这些事物，ChatGPT只能靠自己，而且能凭借自己的能力做得非常出色。 
#+END_QUOTE\
** 📌 [[2024-01-12]]
#+BEGIN_QUOTE
●文章《ChatGPT在做什么？它为何能做到这些？》(“What Is ChatGPT Doing... and Why Does It Work?”)：本书在线版本，包含可运行的代码●文章《初中生能看懂的机器学习》（“Machine Learning for Middle Schoolers”，作者：Stephen Wolfram）：介绍机器学习的基本概念●图书《机器学习入门》（Introduction to Machine Learning，Etienne Bernard 著）：一本关于现代机器学习的指南，包含可运行的代码●网站“Wolfram机器学习”(Wolfram Machine Learning)：阐释Wolfram语言中的机器学习能力●Wolfram U上的机器学习课程：交互式的机器学习课程，适合不同层次的学生学习●文章《如何与AI交流？》（“How Should We Talk to AIs?”，作者：Stephen Wolfram）：2015年的一篇短文，探讨了如何使用自然语言和计算语言与AI交流 
#+END_QUOTE\
** 📌 [[2024-03-12]]
#+BEGIN_QUOTE
在刚刚结束的2023智源大会上，山姆·阿尔特曼很自信地说AGI（artificial general intelligence，通用人工智能）很可能在十年之内到来，需要全球合作解决由此带来的各种问题。而因为共同推动深度学习从边缘到舞台中央而获得图灵奖的三位科学家，意见却明显不同：●杨立昆(Yann LeCun)明确表示GPT代表的自回归大模型存在本质缺陷，需要围绕世界模型另寻新路，所以他对AI的威胁并不担心；●约书亚·本吉奥(Yoshua Bengio)虽然也不认同单靠GPT路线就能通向AGI（他看好将贝叶斯推理与神经网络结合），但承认大模型存在巨大潜力，从第一性原理来看也没有明显的天花板，因此他在呼吁暂停AI开发的公开信上签了字；●压轴演讲的杰弗里·辛顿(Geoffrey Hinton)显然同意自己的弟子伊尔亚·苏茨克维(Ilya Sutskever)提出的“大模型能学到真实世界的压缩表示”的观点，他意识到具备反向传播机制（通俗地说就是内置“知错能改”机制）而且能轻易扩大规模的人工神经网络的智能可能会很快超过人类，因此他也加入到呼吁抵御AI风险的队伍中来。 
#+END_QUOTE\