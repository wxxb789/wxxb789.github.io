:PROPERTIES:
:title: GPT Implemented Using Nu... (highlights)
:author: [[akshay_pachaar on Twitter]]\
:full-title: "GPT Implemented Using Nu..."\
:category: #tweets\
:url: https://twitter.com/akshay_pachaar/status/1627652574315954180\
:END:\

* Highlights first synced by [[Readwise]] [[2023-02-21]]
** GPT implemented using NumPy in like 60 lines of code! 🔥

Let me break it down for to you in this thread: 🧵👇 

![](https://pbs.twimg.com/media/FpaVXg7aIAcm8rL.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652574315954180))
** 1️⃣ gelu

Applies Gaussian Error Linear Units activation function to the input array.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVZw9akAA5Lbx.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652592573767680))
** 2️⃣ softmax

Applies softmax function to the input array.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVaclaAAMPUHh.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652604393308162))
** 3️⃣ layer_norm

Applies layer normalization to the input array.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVbIgaEAE2yqi.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652615843753985))
** 4️⃣ linear

Performs a linear op on the input array.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVbxKaYAALK-O.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652626119802881))
** 5️⃣ ffn

Passes the input array to a feedforward neural network.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVcV0agAUYV_x.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652638602067968))
** 6️⃣ attention

Applies attention mechanism to the input arrays.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVdFiaEAIl6z5.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652649163300865))
** 7️⃣ mha

Applies multi-head attention to the input array.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVdrqaAAEYbsR.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652660534079488))
** 8️⃣ transformer_block

A transformer block that applies multi-head attention and ffn to an input array.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVeUvakAAcd6d.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652672420732930))
** 9️⃣ gpt

A transformer network composed of several transformer blocks.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVfEEaYAAIoNK.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652685561491458))
** 🔟 generate

Generate new tokens given an initial sequence of tokens and a set of parameters.

Check this out 👇 

![](https://pbs.twimg.com/media/FpaVf0DaYAkffDl.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652698647699456))
** Credits: jaymody (GitHub)

Here's the repo ⬇️
https://t.co/0srJ2NFDKa ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652702170927105))
** That's a wrap!

Everyday, I share tutorials around Data Science & Machine Learning.

Find me → @akshay_pachaar ✔️

Like/RT the tweet below to support my work! 🙏 https://t.co/aNPzY2FqWY ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652704867852288))