:PROPERTIES:
:title: GPT Implemented Using Nu... (highlights)
:author: [[akshay_pachaar on Twitter]]\
:full-title: "GPT Implemented Using Nu..."\
:category: #tweets\
:url: https://twitter.com/akshay_pachaar/status/1627652574315954180\
:END:\

* Highlights first synced by [[Readwise]] [[2023-02-21]]
** GPT implemented using NumPy in like 60 lines of code! ğŸ”¥

Let me break it down for to you in this thread: ğŸ§µğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVXg7aIAcm8rL.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652574315954180))
** 1ï¸âƒ£ gelu

Applies Gaussian Error Linear Units activation function to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVZw9akAA5Lbx.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652592573767680))
** 2ï¸âƒ£ softmax

Applies softmax function to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVaclaAAMPUHh.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652604393308162))
** 3ï¸âƒ£ layer_norm

Applies layer normalization to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVbIgaEAE2yqi.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652615843753985))
** 4ï¸âƒ£ linear

Performs a linear op on the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVbxKaYAALK-O.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652626119802881))
** 5ï¸âƒ£ ffn

Passes the input array to a feedforward neural network.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVcV0agAUYV_x.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652638602067968))
** 6ï¸âƒ£ attention

Applies attention mechanism to the input arrays.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVdFiaEAIl6z5.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652649163300865))
** 7ï¸âƒ£ mha

Applies multi-head attention to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVdrqaAAEYbsR.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652660534079488))
** 8ï¸âƒ£ transformer_block

A transformer block that applies multi-head attention and ffn to an input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVeUvakAAcd6d.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652672420732930))
** 9ï¸âƒ£ gpt

A transformer network composed of several transformer blocks.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVfEEaYAAIoNK.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652685561491458))
** ğŸ”Ÿ generate

Generate new tokens given an initial sequence of tokens and a set of parameters.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVf0DaYAkffDl.jpg) ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652698647699456))
** Credits: jaymody (GitHub)

Here's the repo â¬‡ï¸
https://t.co/0srJ2NFDKa ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652702170927105))
** That's a wrap!

Everyday, I share tutorials around Data Science & Machine Learning.

Find me â†’ @akshay_pachaar âœ”ï¸

Like/RT the tweet below to support my work! ğŸ™ https://t.co/aNPzY2FqWY ([View Tweet](https://twitter.com/akshay_pachaar/status/1627652704867852288))