:PROPERTIES:
:title: 🔭 How to Reduce \#LLM Gen... (highlights)
:author: [[ShayneRedford on Twitter]]\
:full-title: "🔭 How to Reduce \#LLM Gen..."\
:category: #tweets\
:url: https://twitter.com/ShayneRedford/status/1628068629983150080\
:END:\

* Highlights first synced by [[Readwise]] [[2023-02-22]]
** 🔭 How to reduce #llm generation toxicity/bias?

I'm surprised this finding hasn't received any attention:

Instruction Tuning (e.g. Flan, T0) reduces toxic generations A LOT ✨ w/o any Human Feedback ✨.

➡️ I.e. #ChatGPT-esque Human values alignment w/o human feedback.

1/ 

![](https://pbs.twimg.com/media/FpgPyQLaMAAu79D.jpg) 

![](https://pbs.twimg.com/media/FpgPyhWaEAEfsJe.jpg) ([View Tweet](https://twitter.com/ShayneRedford/status/1628068629983150080))
** Buried in the Scaling Flan Appendix C:

1⃣ Generating toxic responses to non-toxic prompts (🔻 44% ➡️ 18%)
2⃣ Generating toxic responses to prompts on identity groups (🔻🔻)
3⃣ Ability to detect toxic content 0-shot (🔺14%+)

📜: https://t.co/lvYsx8kPfF

2/ ([View Tweet](https://twitter.com/ShayneRedford/status/1628068633904824320))
** 🌟 Finding 1⃣ 🌟

RealToxicityPrompts measures how often toxic / non-toxic prompts trigger biased/toxic/profane responses (rated by Perspective API).

RTP 📜: https://t.co/UcGsE9d8d0, @samgehman @ssgrn et al

Figures: Flan-PaLM's responses eval less toxic than PaLM's

3/ 

![](https://pbs.twimg.com/media/FpgPzE8agAAwcBV.png) 

![](https://pbs.twimg.com/media/FpgPzWVaMAAV0ag.jpg) ([View Tweet](https://twitter.com/ShayneRedford/status/1628068646508728320))
** 🌟 Finding 2⃣ 🌟

Responses to prompts for identity groups are systematically less toxic for Flan-PaLM than PaLM.

Figures: Religion/Gender/Ethnicity toxicities lower (L), though tail distribution toxicities aren't solved (R).

4/ 

![](https://pbs.twimg.com/media/FpgPz5sagAIiu7Z.jpg) 

![](https://pbs.twimg.com/media/FpgP0QJaMAEQ8MP.jpg) ([View Tweet](https://twitter.com/ShayneRedford/status/1628068662275092482))
** 🌟 Finding 3⃣ 🌟

LLMs' ability to identify whether text is toxic or not, using Civil Comments, is significantly improved (perhaps less surprising).

Civil Comments 📜: https://t.co/IjLhkDqOWI

Table: Flan-PaLM > PaLM by ~13%+ for 0-shot, and ~5%+ for 10-shot.

5/ 

![](https://pbs.twimg.com/media/FpgP0x6aAAAhKyC.png) ([View Tweet](https://twitter.com/ShayneRedford/status/1628068670068133889))
** Why is this important?

New work shows larger models can harbor more bias (📜: https://t.co/TTKQC1MQot Ganguli, @AmandaAskell, @nschiefer et al.)

And larger models may hallucinate non-factual info more (https://t.co/cs8z1ka8Sz)

6/ 

![](https://pbs.twimg.com/media/FpgP1OwaYAAyb-E.jpg) 

![](https://pbs.twimg.com/media/FpgP1hNaAAYiH2b.jpg) ([View Tweet](https://twitter.com/ShayneRedford/status/1628068685456998400))
** 🌟 Take-aways 🌟

Much of the discussion on "alignment to human values" has centered on collecting human feedback signals to model responses.

But the Flan Collection is simply NLP tasks framed as instructions...

7/ ([View Tweet](https://twitter.com/ShayneRedford/status/1628068689902972929))
** 🌟 Take-aways 🌟

This isn't to say human feedback doesn't provide stronger benefits, but significant toxicity reduction (or better #AISafety) may be achievable with the tools we already have (simple instruction tuning), even without new large-scale collection efforts.

8/ ([View Tweet](https://twitter.com/ShayneRedford/status/1628068692562186242))
** 🌟 Take-aways 🌟

TLDR: Care about bias/toxicity? Use your favorite instruction-tuned model.

(Like Flan-T5: https://t.co/p7gVis9lHM!)

9/ ([View Tweet](https://twitter.com/ShayneRedford/status/1628068695154229249))
** 🌟 Limitations 🌟

➡️ Toxicity evals are imperfect and not comprehensive!
➡️ These evals are only English and western-centric
➡️ Review the paper for important details

10/ ([View Tweet](https://twitter.com/ShayneRedford/status/1628068697763115009))
** Extra credit goes to Kevin Robinson, @Hou_Le, @m_pellat, Dasha Valter, @acastroros who ran these evals.

11/11 ([View Tweet](https://twitter.com/ShayneRedford/status/1628068700443246597))
** And thank you to @_jasonwei and @YiTayML for feedback on this thread! ([View Tweet](https://twitter.com/ShayneRedford/status/1628068917708050433))