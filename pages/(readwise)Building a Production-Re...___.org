:PROPERTIES:
:title: (readwise)Building a Production-Re...
:END:

:PROPERTIES:
:author: [[llama_index on Twitter]]
:full-title: "Building a Production-Re..."
:category: [[tweets]]
:url: https://twitter.com/llama_index/status/1673451316398653440
:image-url: https://pbs.twimg.com/profile_images/1623505166996742144/n-PNQGgd.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-05]]
** üìå
#+BEGIN_QUOTE
Building a production-ready LLM app is hard:
üìÑ How to load, parse, embed thousands of docs?
‚öôÔ∏è How to deploy to prod?

We‚Äôre incredibly excited to collab with @anyscalecompute: Ray can make LlamaIndex 10x faster + easily deployable to a prod server ‚ö°Ô∏è

https://t.co/jpWdNTqfzn 
#+END_QUOTE
    date:: [[2023-06-27]]
*** from _Building a Production-Re..._ by @llama_index on Twitter
*** [View Tweet](https://twitter.com/llama_index/status/1673451316398653440)
** üìå
#+BEGIN_QUOTE
The core @raydistributed toolkit is awesome for easily parallelizing different tasks. In addition, Ray Serve makes it super easy to deploy our query engines to production.

Check out our YouTube video! üé¨

HUGE s/o to @AmogKamsetty for the help.

https://t.co/JAY5xfE66H 
#+END_QUOTE
    date:: [[2023-06-27]]
*** from _Building a Production-Re..._ by @llama_index on Twitter
*** [View Tweet](https://twitter.com/llama_index/status/1673451318634217473)