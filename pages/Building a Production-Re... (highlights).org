:PROPERTIES:
:title: Building a Production-Re... (highlights)
:END:
:PROPERTIES:
:author: [[llama_index on Twitter]]
:full-title: "Building a Production-Re..."
:category: [[tweets]]
:url: https://twitter.com/llama_index/status/1673451316398653440
:END:

* Highlights first synced by [[Readwise]] [[2023\-06\-27]]
** Building a production\-ready LLM app is hard:
üìÑ How to load, parse, embed thousands of docs?
‚öôÔ∏è How to deploy to prod?

We‚Äôre incredibly excited to collab with @anyscalecompute: Ray can make LlamaIndex 10x faster + easily deployable to a prod server ‚ö°Ô∏è

https://t.co/jpWdNTqfzn ([View Tweet](https://twitter.com/llama_index/status/1673451316398653440))
** The core @raydistributed toolkit is awesome for easily parallelizing different tasks. In addition, Ray Serve makes it super easy to deploy our query engines to production.

Check out our YouTube video! üé¨

HUGE s/o to @AmogKamsetty for the help.

https://t.co/JAY5xfE66H ([View Tweet](https://twitter.com/llama_index/status/1673451318634217473))