:PROPERTIES:
:title: Claude 2.1 (200K Tokens)... (highlights)
:END:

:PROPERTIES:
:author: [[GregKamradt on Twitter]]
:full-title: "Claude 2.1 (200K Tokens)..."
:category: [[tweets]]
:url: https://twitter.com/GregKamradt/status/1727018183608193393
:image-url: https://pbs.twimg.com/profile_images/1467896309453570052/BGy5XYVQ.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-11-22]]
** üìå
** #+BEGIN_QUOTE
** Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall

We all love increasing context lengths - but what's performance like?

Anthropic reached out with early access to Claude 2.1 so I repeated the ‚Äúneedle in a haystack‚Äù analysis I did on GPT-4

Here's what I found:

Findings:
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** Code + overview video of the analysis

https://t.co/v64RHinrNW  ([View Tweet](https://twitter.com/GregKamradt/status/1727018572558581938))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** Anthropic Launch of Claude 2.1

https://t.co/XnAI7bFEI9  ([View Tweet](https://twitter.com/GregKamradt/status/1727018644964880507))
** #+END_QUOTE