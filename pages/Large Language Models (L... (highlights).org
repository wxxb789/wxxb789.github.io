:PROPERTIES:
:title: Large Language Models (L... (highlights)
:author: [[@cwolferesearch on Twitter]]
:full-title: "Large Language Models (L..."
:category: #tweets
:url: https://twitter.com/cwolferesearch/status/1606445323336966146
:END:

* Highlights first synced by [[Readwise]] [[2022-12-25]]
** Large language models (LLMs) are great at task-agnostic, few-shot learning. Recent LLMs extend upon these skills by assisting humans with open-ended generation and brainstorming in specific domains. Here are some notable examples... ðŸ§µ[1/10] ([View Tweet](https://twitter.com/cwolferesearch/status/1606445323336966146))
** Instruct/ChatGPT refine a pre-trained LLM via reinforcement learning from human feedback (RLHF) to follow a user's instructions and avoid harmful behavior (e.g., incorrect or toxic output). These LLMs can debug code, explain scientific concepts, and more. [2/10] 

![](https://pbs.twimg.com/media/FksxyBoXEAAh92Z.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445324620500995))
** After using human annotators to collect examples of high-quality, safe, and grounded dialogue, LaMDA fine-tunes a pre-trained LLM to measure/predict these properties, then filters its output accordingly. The result is an interesting and powerful dialogue model. [3/10] 

![](https://pbs.twimg.com/media/Fkszis0XEAMKa_G.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445326088511489))
** Sparrow is similar to LaMDA, but it uses an RLHF framework instead of supervised fine-tuning to improve the LLMs information-seeking dialogue capabilities. The model is also taught to use an external google search feature to support its claims with sources. [4/10] 

![](https://pbs.twimg.com/media/Fks1eTBWIAEtCOe.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445327392903169))
** Galactica is an LLM that is pre-trained over a curated corpus of scientific knowledge. It handles numerous data modalities from equations to DNA sequences and is an interesting proof-of-concept for using LLMs to distill and reason over lots of scientific literature. [5/10] 

![](https://pbs.twimg.com/media/Fks2w9QXkAAMkFS.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445328688889858))
** PubMedGPT is similar to Galactica, but it pre-trains a smaller LLM over a corpus of abstracts and papers from PubMed. The resulting model achieves state-of-the-art results on the US medical licensing exam, showing that smaller, domain-specific LLMs are really useful. [6/10] 

![](https://pbs.twimg.com/media/Fks4Aa7WAAAxJLv.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445330085601281))
** Codex is the LLM that powers GitHub Copilot. It is pre-trained over a large corpus of Python code from Github and further fine-tuned on a curated set of programming problems. Codex is really effective at generating working Python scripts from an associated docstring. [7/10] 

![](https://pbs.twimg.com/media/Fks5V5_WQAMcjk2.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445331067113474))
** Dramatron is an LLM that specializes in co-writing theater scripts and screenplays with humans. It follows a hierarchical process for generating coherent stories and was deemed useful to the creative process in a user study with 15 theatre/film professionals. [8/10] 

![](https://pbs.twimg.com/media/Fks6rYqWQAccPxL.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445333487181825))
** Overall, recent LLMs applications have become less generic and moved towards specialized use in particular domains. Domain-specific LLMs can be created with techniques like:

1. LM pre-training over domain-specific data
2. RLHF
3. Supervised fine-tuning

[9/10] 

![](https://pbs.twimg.com/media/Fks74EzWAAYxlA9.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1606445334548385794))
** I will be summarizing all of these models (and the techniques/procedures used to create them) in the next edition of my newsletter. Feel free to subscribe or check out the several recent overviews I have written about LLMs at the link below!

https://t.co/qmA4dNnsRF

[10/10] ([View Tweet](https://twitter.com/cwolferesearch/status/1606445335794122754))