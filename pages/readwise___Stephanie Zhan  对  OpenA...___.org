:PROPERTIES:
:title: readwise/Stephanie Zhan  对  OpenA...
:END:


* metadata
:PROPERTIES:
:author: [[op7418 on Twitter]]
:full-title: "Stephanie Zhan  对  OpenA..."
:category: [[tweets]]
:url: https://twitter.com/op7418/status/1773194599756173716
:image-url: https://pbs.twimg.com/profile_images/1636981205504786434/xDl77JIw.jpg
:END:

* Highlights first synced by [[Readwise]] [[2024-04-08]]
** 📌 [[2024-03-28]]
#+BEGIN_QUOTE
Stephanie Zhan  对  OpenAI 前研究员 Karpathy 进行了访谈。谈论内容有 LLM 操作系统、创业公司如何同大公司竞争、从马斯克那里学到了什么、对AI未来的展望等。

只能说还得是安德烈，半小时的内容比 Sam 两小时的采访有效信息多多了。

我翻译了视频并且整理了文字版本感兴趣可以看看。

AndreyKarpathy在OpenAI的早期经历：

Andrej回忆了他在OpenAI最初的办公室工作的经历。那是在旧金山的一个办公室,Andrej在那里工作了大约两年时间。办公室的楼下有一家巧克力工厂,所以空气中总是飘着好闻的巧克力味道。当时OpenAI的团队规模大约在10到20人左右。

OpenAI在AI生态系统中的角色和其他公司的机会：

OpenAI目前正在努力构建一个类似于操作系统的东西,被Andrej称为"LLMOS"。这个系统将为不同的公司和垂直领域提供定制化的应用。就像Windows操作系统自带一些默认的应用程序一样,LLMOS也会有一些默认的应用。但同时,它也会支持一个丰富的第三方应用生态,针对经济中的各个不同领域。其他公司要想在这个生态中获得一席之地,需要一些时间来找到如何与这个新的基础设施合作的最佳方式。

AI模型发展的关键因素:规模和其他：

在AI模型的发展中,模型的规模无疑是最重要的因素。但除了规模之外,还有很多其他同样重要的因素,例如数据的质量、算法的选择、训练的技巧等等。规模可以看作是模型性能的上限,而这些其他因素则决定了我们能在多大程度上逼近这个上限。此外,强大的计算基础设施和优秀的人才也是不可或缺的。单靠资金和计算资源是不够的,还需要在算法、工程等方面有深厚积累的团队。

从Elon Musk身上学到的管理经验：

Andrej曾与Elon Musk有过密切的合作,从中学到了宝贵的管理经验。Elon喜欢小而精干的技术团队,他会积极地提拔优秀的员工,同时也会毫不犹豫地淘汰绩效不达标的员工。Elon希望打造一个紧张、忙碌、充满激情的工作环境。他不喜欢冗长的会议,鼓励员工高效地沟通。Elon非常重视工程师的意见,经常会直接与工程师对话,而不是只听取高管的汇报。他也不怕快速做出决定,迅速解决问题和消除前进的障碍。

对AI未来的展望和个人使命：

谈到他个人最关注的事情时,Andrej表示比起任何一家具体的公司,他更关心整个AI生态系统的健康发展。他希望看到一个欣欣向荣的AI创业生态,而不是被几大巨头垄断的局面。未来可能会呈现一种类似Windows和Linux并存的状态,即会有一些主导的封闭平台,但同时也会有开放和开源的选择。

开源AI生态的现状和发展：

目前所谓的开源AI模型往往只是开放了训练后的模型权重,并没有提供数据、代码等一整套训练基础设施,因此还不能算是真正意义上的开源。真正的开源需要对整个流程进行开放,让社区可以基于这些积木进行创新。一些大公司可以考虑更多地支持开源生态的发展,这不仅有利于推动AI民主化,也有利于整个产业的进步。

模型组合性方面的进展和思考：

与传统的软件开发相比,当前的神经网络模型在组合性方面还有所欠缺。不过也有一些组合使用的例子,比如可以将预训练好的模型组件进行组装,然后在下游任务上进行微调。但总的来说,目前还没有一套系统性的模型组合方法论。未来可能需要在模块化、标准化等方面做更多的研究。

物理学家型AI的构想和挑战：

构建一个类似物理学家的AI模型是一个非常有吸引力的想法,但目前的模型要达到这个目标还有很长的路要走。当前模型最大的问题是缺乏自主学习和思考的能力,更多的是对训练数据的模仿。它们需要学会自主练习解题、评估自身的局限性,形成连贯的世界模型。强化学习可能是一个值得探索的方向,但目前在这方面的尝试还非常初步,效果难以令人满意。要实现一个真正的"科学家型AI",我们还需要在算法、架构等方面取得重大突破。

成本和性能的权衡：

在实际的开发中,我们通常会面临模型性能和计算成本之间的权衡。一般的策略是先追求尽可能好的性能指标,而把成本优化放到后面。比如我们可以先用尽量好的模型去生成高质量的训练数据,然后再训练一个更小更便宜的模型去拟合这些数据。总的来说,我们应该遵循"先让系统work起来,再去考虑优化"的原则,即功能第一,优化第二。

Transformer架构的现状和未来：

Transformer架构的出现令很多人感到意外,它展现了远超预期的适应性和性能。但我们也不能完全排除未来出现颠覆性的新架构的可能性。架构的设计需要综合考虑硬件特性、优化目标等多方面因素。以Transformer为例,它的设计就充分考虑了GPU并行计算的特点。未来的架构创新可能会来自于对新的硬件、新的应用场景的深入思考。总之,尽管Transformer已经很成功了,但探索架构创新的大门依然敞开着。<video controls><source src="https://video.twimg.com/amplify_video/1773193068533227520/pl/_4raHRmDZ7nT-xXG.m3u8?tag=14&container=cmaf" type="application/x-mpegURL"><source src="https://video.twimg.com/amplify_video/1773193068533227520/vid/avc1/480x270/RQBMY3k_idHdaMve.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/amplify_video/1773193068533227520/vid/avc1/640x360/zR278_Y2ok7uUOOy.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/amplify_video/1773193068533227520/vid/avc1/1280x720/wzmfKdTRNeGwHc4n.mp4?tag=14" type="video/mp4">Your browser does not support the video tag.</video> 
#+END_QUOTE\