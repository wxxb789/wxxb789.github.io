:PROPERTIES:
:title: readwise/Building a Production-Re...
:END:


* metadata
:PROPERTIES:
:author: [[llama_index on Twitter]]
:full-title: "Building a Production-Re..."
:category: [[tweets]]
:url: https://twitter.com/llama_index/status/1673451316398653440
:image-url: https://pbs.twimg.com/profile_images/1623505166996742144/n-PNQGgd.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** ğŸ“Œ [[2023-06-27]]
#+BEGIN_QUOTE
Building a production-ready LLM app is hard:
ğŸ“„ How to load, parse, embed thousands of docs?
âš™ï¸ How to deploy to prod?

Weâ€™re incredibly excited to collab with @anyscalecompute: Ray can make LlamaIndex 10x faster + easily deployable to a prod server âš¡ï¸

https://t.co/jpWdNTqfzn 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-27]]
#+BEGIN_QUOTE
The core @raydistributed toolkit is awesome for easily parallelizing different tasks. In addition, Ray Serve makes it super easy to deploy our query engines to production.

Check out our YouTube video! ğŸ¬

HUGE s/o to @AmogKamsetty for the help.

https://t.co/JAY5xfE66H 
#+END_QUOTE\