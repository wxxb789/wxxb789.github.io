:PROPERTIES:
:title: [1_9] Large Language Mod... (highlights)
:END:

:PROPERTIES:
:author: [[yanda_chen_ on Twitter]]
:full-title: "[1/9] Large Language Mod..."
:category: [[tweets]]
:url: https://twitter.com/yanda_chen_/status/1681412273758408704
:END:

* Highlights first synced by [[Readwise]] [[2023-07-19]]
** ğŸ“Œ
** #+BEGIN_QUOTE
** [1/9] Large Language Models (LLMs) can mimic humans to explain human decisions. But can they explain THEMSELVEs? How to evaluate explanations along this axis? Check out our work â€œDo Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanationsâ€! 

![](https://pbs.twimg.com/media/F1WTg7gakAAhBVB.jpg) ([View Tweet](https://twitter.com/yanda_chen_/status/1681412273758408704))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [2/9] Say a model answers â€œyesâ€ to â€œCan eagles fly?â€ and explains that â€œall birds can flyâ€. Humans would infer from the explanation that it also answers â€œyesâ€ to â€œCan penguins fly?â€ (counterfactual). If the explanation is precise, the model's ans should match humans' expectations ([View Tweet](https://twitter.com/yanda_chen_/status/1681412355090169856))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [3/9] We propose to evaluate the counterfactual simulatability of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. 

![](https://pbs.twimg.com/media/F1WTuP0akAAjRxr.jpg) ([View Tweet](https://twitter.com/yanda_chen_/status/1681412566470500355))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [4/9] We propose two metrics for explanations: generality and precision. Generality tracks the diversity of counterfactuals relevant to the explanation. Precision tracks the fraction of counterfactuals where humans' inference matches the model's output. ([View Tweet](https://twitter.com/yanda_chen_/status/1681412647722553344))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [5/9] We benchmark the counterfactual simulatability of two LLMsâ€”GPT-3.5 and GPT-4, and two explanation methodsâ€”CoT (Chain of Thought) and Post-Hoc (explain after the output), on two tasksâ€”multi-hop factual reasoning (StrategyQA) and reward modeling (Stanford Human Preference). ([View Tweet](https://twitter.com/yanda_chen_/status/1681412740416704512))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [6/9] Both GPT-3 and GPT4â€™s explanations have low precision (80% for binary classification). CoT does not substantially outperform Post-Hoc. 

![](https://pbs.twimg.com/media/F1WUDtDaIAAqWSI.jpg) ([View Tweet](https://twitter.com/yanda_chen_/status/1681412847476310017))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [7/9] We study how precision relates to plausibility (humans' preference of an explanation based on factual and logical correctness). Precision does not correlate with plausibility (corr: +0.01). Thus, naively optimizing human approvals (e.g., RLHF) might not improve precision. ([View Tweet](https://twitter.com/yanda_chen_/status/1681412918733328385))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [8/9] Finally, we show that our two metrics precision and generality do not correlate with each other (correlation +0.020). Thus, both generality and precision are important in evaluating and optimizing explanations. ([View Tweet](https://twitter.com/yanda_chen_/status/1681412980083417088))
** #+END_QUOTE
** ğŸ“Œ
** #+BEGIN_QUOTE
** [9/9] Paper: https://t.co/qFQUE1KisW
Coauthors: @ZhongRuiqi, @narutatsuri, @henryzhao4321, @hhexiy, @JacobSteinhardt, @Zhou_Yu_AI, Kathleen McKeown ([View Tweet](https://twitter.com/yanda_chen_/status/1681414012054167552))
** #+END_QUOTE