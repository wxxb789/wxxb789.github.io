:PROPERTIES:
:title: readwise/给我的神经网络实现加了个卷积层（代码不复杂，但时...
:END:


* metadata
:PROPERTIES:
:author: [[cloudwu on Twitter]]
:full-title: "给我的神经网络实现加了个卷积层（代码不复杂，但时..."
:category: [[tweets]]
:url: https://twitter.com/cloudwu/status/1638786229176262657
:image-url: https://pbs.twimg.com/profile_images/1385692491/me2.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-20]]
** 📌
#+BEGIN_QUOTE
给我的神经网络实现加了个卷积层（代码不复杂，但时间都花在自己推导卷积层的误差反向传播算法上了），果然准确率大幅度提高到了接近 99% 。根据我最近读的几本书，调整一下训练方法应该还有进一步提升的空间。 https://t.co/eUSZ1n3Xs0 
#+END_QUOTE
    date:: [[2023-03-23]]
*** from _给我的神经网络实现加了个卷积层（代码不复杂，但时..._ by @cloudwu on Twitter
*** [[https://twitter.com/cloudwu/status/1638786229176262657][View Tweet]]
** 📌
#+BEGIN_QUOTE
这个卷积网络的训练时间比之前的简单全连接网络高了一个数量级。从几十秒变成了接近 20 分钟。我觉得 mini batch 那个步骤完全可以并行，或许我改到 ltask 中多线程运行或许可以提高个 5,6 倍的速度。但好像这种优化对于学习没什么意义。ps. 前几天有人说用 cpu 针对 mnist 训练要几个小时？ 
#+END_QUOTE
    date:: [[2023-03-23]]
*** from _给我的神经网络实现加了个卷积层（代码不复杂，但时..._ by @cloudwu on Twitter
*** [[https://twitter.com/cloudwu/status/1638787113419411456][View Tweet]]