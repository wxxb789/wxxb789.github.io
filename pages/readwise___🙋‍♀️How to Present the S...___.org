:PROPERTIES:
:title: readwise/ğŸ™‹â€â™€ï¸How to Present the S...
:END:


* metadata
:PROPERTIES:
:author: [[WeijiaShi2 on Twitter]]
:full-title: "ğŸ™‹â€â™€ï¸How to Present the S..."
:category: [[tweets]]
:url: https://twitter.com/WeijiaShi2/status/1605307966109863936
:image-url: https://pbs.twimg.com/profile_images/1605693076600541185/fXXYNWhx.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
ğŸ™‹â€â™€ï¸How to present the same text in diff. tasks/domains as diff. embeddings W/O training?

We introduce InstructorğŸ‘¨â€ğŸ«, an instruction-finetuned embedder that can generate text embeddings tailored to any task given the task instructionâ¡ï¸sota on 7âƒ£0âƒ£tasksğŸ‘‡!

https://t.co/mD8UeVg4WE 

![](https://pbs.twimg.com/media/Fkcx8AlUEAMxUXj.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
We first annotate instructions for 330 diverse tasks and train InstructorğŸ‘¨â€ğŸ« on this multitask mixture

A single InstructorğŸ‘¨â€ğŸ« model can achieve sota on 70 embedding evaluation tasks:
1âƒ£ Retrieval 
2âƒ£TextEval
3âƒ£Clustering
4âƒ£Prompt Retrieval
5âƒ£Classification
6âƒ£STS
7âƒ£Reranking
8âƒ£... 

![](https://pbs.twimg.com/media/FkcyAatVQAEfVAg.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
Smaller model but best performance

InstructorğŸ‘¨â€ğŸ« (335M), while having >10x fewer parameters than the previous best model (4.8B), achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. 

![](https://pbs.twimg.com/media/FkcyD12VEAAvPBi.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
Instructions Enable Diverse Training

Finetuning with instructions allows InstructorğŸ‘¨â€ğŸ« to benefit from the diversity of 330 datasets, whereas simply training on those datasets alone leads to degraded performance. 

![](https://pbs.twimg.com/media/FkcyHqfVEAAITXB.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
Instruction-finetuning on a large amount of datasets with diverse task instructions improves the robustness of InstructorğŸ‘¨â€ğŸ« to instruction paraphrases (i.e., smaller performance gaps between best- and worst-performing instructions) 

![](https://pbs.twimg.com/media/FkcyMxDVEAAcy9u.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
In addition, by making instruction more detailed (Left) and model size larger (Right), the performance of InstructorğŸ‘¨â€ğŸ« is consistently improved. 

![](https://pbs.twimg.com/media/FkcyRFvVUAArLlB.png) 

![](https://pbs.twimg.com/media/FkcyRGIUUAAEK3g.png) 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
Instructions Mitigate Domain Shifts

Instruction-finetuned InstructorğŸ‘¨â€ğŸ« helps more on unseen domains: geography, biology and civil comments. Domain-specific datasets benefit particularly from instruction finetuning. 

![](https://pbs.twimg.com/media/FkcyUvPUAAApvq0.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
Led by @hongjin_su and @WeijiaShi2, joint work with @wittgen_ball , @yizhongwyz, @huyushi98, Mari, @scottyih, @nlpnoah, @LukeZettlemoyer, and @taoyds from @uwnlp, @allen_ai and @MetaAI.

Thanks @Muennighoff and @Nils_Reimers for the nice MTEB code and data. It did save our life! 
#+END_QUOTE\
** ğŸ“Œ [[2022-12-21]]
#+BEGIN_QUOTE
InstructorğŸ‘¨â€ğŸ« embedding model is on @huggingface
 ğŸ¤—: https://t.co/exB63SYxrM! 

It is very simple to use! 
#+END_QUOTE\