:PROPERTIES:
:title: The Contrastive Language... (highlights)
:author: [[@cwolferesearch on Twitter]]
:full-title: "The Contrastive Language..."
:category: #tweets
:url: https://twitter.com/cwolferesearch/status/1608207404965195776
:END:

* Highlights first synced by [[Readwise]] [[2022-12-31]]
** The Contrastive Language-Image Pre-Training (CLIP) model is a visual-linguistic deep learning model that became popular for zero-shot classifying ImageNet with >76% accuracy. Recently, researchers discovered that CLIP also has SOTA fine-tuning performanceâ€¦ ðŸ§µ[1/8] ([View Tweet](https://twitter.com/cwolferesearch/status/1608207404965195776))
** CLIP is a simple model with two components:

1. Image encoder (transformer or CNN)
2. Text encoder (transformer)

Taking image-caption pairs as input, CLIP outputs an embedding of this visual-linguistic data that jointly describes visual and textual properties of the data. [2/8] 

![](https://pbs.twimg.com/media/FlF99iTX0AAeNdd.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1608207406160572417))
** CLIP is trained in a weakly supervised manner by classifying correct captions across a large image-caption pairs dataset. After training, CLIP produces high-quality embeddings and can classify images by pairing them with their most likely (textual) class. [3/8] 

![](https://pbs.twimg.com/media/FlF-YylXEAAvR57.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1608207408253505537))
** Despite its impressive zero-shot performance, CLIP seemed to not perform that well when directly fine-tuned for image classification. Rather, CLIP embeddings were commonly used as a target for training separate models via masked image modeling (MIM). [4/8] 

![](https://pbs.twimg.com/media/FlF-nB5WAAETo4E.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1608207409922572295))
** Recent research explored directly fine-tuning CLIP for image classification. By modifying/solidifying the fine-tuning approach a bit, we can show that a fine-tuned CLIP actually outperforms a ton of strong baselines and reaches SOTA performance on ImageNet! [5/8] 

![](https://pbs.twimg.com/media/FlF-8ldXgAELFWM.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1608207412175196161))
** There are a few keys to making this work:

- tuning hyperparameters (correct LR/epochs, layer-wise LR decay, etc.)
- taking an EMA of model weights (reduces overfitting)
- using correct data augmentation (no MixUp/CutMix, use RandAugment+RandomErase)

[6/8] 

![](https://pbs.twimg.com/media/FlF_L6bXwAEaPd0.jpg) ([View Tweet](https://twitter.com/cwolferesearch/status/1608207414087540738))
** TL;DR: CLIP is great at zero-shot image classification and even better when fine-tuned!

Thanks @davisblalock for pointing out this paper (his newsletter is awesome, please subscribe!). To learn about CLIP, check out my overview:

https://t.co/kX8Dv8xw7z 

[7/8] ([View Tweet](https://twitter.com/cwolferesearch/status/1608207416126054402))
** Here are the relevant papers:
1. CLIP: https://t.co/Q7UwHXInQT 
2. CLIP+MIM (see also references in the paper below!): https://t.co/cYId2LM2rB 
3. CLIP fine-tuning: https://t.co/n3ZnmhSqRL

[8/8] ([View Tweet](https://twitter.com/cwolferesearch/status/1608207417401135107))