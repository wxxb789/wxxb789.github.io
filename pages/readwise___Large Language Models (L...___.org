:PROPERTIES:
:title: readwise/Large Language Models (L...
:END:


* metadata
:PROPERTIES:
:author: [[cwolferesearch on Twitter]]
:full-title: "Large Language Models (L..."
:category: [[tweets]]
:url: https://twitter.com/cwolferesearch/status/1606445323336966146
:image-url: https://pbs.twimg.com/profile_images/1715212547215802368/tqxfSqh3.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-20]]
** ðŸ“Œ
#+BEGIN_QUOTE
Large language models (LLMs) are great at task-agnostic, few-shot learning. Recent LLMs extend upon these skills by assisting humans with open-ended generation and brainstorming in specific domains. Here are some notable examples... ðŸ§µ[1/10] 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445323336966146][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Instruct/ChatGPT refine a pre-trained LLM via reinforcement learning from human feedback (RLHF) to follow a user's instructions and avoid harmful behavior (e.g., incorrect or toxic output). These LLMs can debug code, explain scientific concepts, and more. [2/10] 

![](https://pbs.twimg.com/media/FksxyBoXEAAh92Z.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445324620500995][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
After using human annotators to collect examples of high-quality, safe, and grounded dialogue, LaMDA fine-tunes a pre-trained LLM to measure/predict these properties, then filters its output accordingly. The result is an interesting and powerful dialogue model. [3/10] 

![](https://pbs.twimg.com/media/Fkszis0XEAMKa_G.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445326088511489][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Sparrow is similar to LaMDA, but it uses an RLHF framework instead of supervised fine-tuning to improve the LLMs information-seeking dialogue capabilities. The model is also taught to use an external google search feature to support its claims with sources. [4/10] 

![](https://pbs.twimg.com/media/Fks1eTBWIAEtCOe.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445327392903169][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Galactica is an LLM that is pre-trained over a curated corpus of scientific knowledge. It handles numerous data modalities from equations to DNA sequences and is an interesting proof-of-concept for using LLMs to distill and reason over lots of scientific literature. [5/10] 

![](https://pbs.twimg.com/media/Fks2w9QXkAAMkFS.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445328688889858][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
PubMedGPT is similar to Galactica, but it pre-trains a smaller LLM over a corpus of abstracts and papers from PubMed. The resulting model achieves state-of-the-art results on the US medical licensing exam, showing that smaller, domain-specific LLMs are really useful. [6/10] 

![](https://pbs.twimg.com/media/Fks4Aa7WAAAxJLv.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445330085601281][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Codex is the LLM that powers GitHub Copilot. It is pre-trained over a large corpus of Python code from Github and further fine-tuned on a curated set of programming problems. Codex is really effective at generating working Python scripts from an associated docstring. [7/10] 

![](https://pbs.twimg.com/media/Fks5V5_WQAMcjk2.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445331067113474][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Dramatron is an LLM that specializes in co-writing theater scripts and screenplays with humans. It follows a hierarchical process for generating coherent stories and was deemed useful to the creative process in a user study with 15 theatre/film professionals. [8/10] 

![](https://pbs.twimg.com/media/Fks6rYqWQAccPxL.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445333487181825][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Overall, recent LLMs applications have become less generic and moved towards specialized use in particular domains. Domain-specific LLMs can be created with techniques like:

1. LM pre-training over domain-specific data
2. RLHF
3. Supervised fine-tuning

[9/10] 

![](https://pbs.twimg.com/media/Fks74EzWAAYxlA9.jpg) 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445334548385794][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
I will be summarizing all of these models (and the techniques/procedures used to create them) in the next edition of my newsletter. Feel free to subscribe or check out the several recent overviews I have written about LLMs at the link below!

https://t.co/qmA4dNnsRF

[10/10] 
#+END_QUOTE
    date:: [[2022-12-24]]
*** from _Large Language Models (L..._ by @cwolferesearch on Twitter
*** [[https://twitter.com/cwolferesearch/status/1606445335794122754][View Tweet]]