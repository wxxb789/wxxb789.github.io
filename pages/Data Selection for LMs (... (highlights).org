:PROPERTIES:
:title: Data Selection for LMs (... (highlights)
:author: [[sangmichaelxie on Twitter]]
:full-title: "Data Selection for LMs (..."
:category: #tweets
:url: https://twitter.com/sangmichaelxie/status/1623397365443960832
:END:

* Highlights first synced by [[Readwise]] [[2023-02-09]]
** Data selection for LMs (GPT-3, PaLM) is done with heuristics that select data by training a classifier for high-quality text. Can we do better?

Turns out we can boost downstream GLUE acc by 2+% by adapting the classic importance resampling algorithm..

https://t.co/8PQtPFiCol
üßµ 

![](https://pbs.twimg.com/media/FoaWcFZacAAlttT.jpg) ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397365443960832))
** Before diving in - we've uploaded pre-filtered datasets to HuggingFace that were selected from The Pile for high-quality text! 
You can also use the code to select custom language modeling datasets from The Pile. 

Pre-filtered Pile data and code: https://t.co/HWrR9LRnpF ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397371622064132))
** Data selection typically involves filtering a large source of raw data towards some desired target distribution, whether it's high-quality/formal text (e.g., Wikipedia + books) for general-domain LMs like GPT-3 or domain-specific data for specialized LMs like Codex. ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397376101691393))
** Because of the large scale of raw text data, existing methods (GPT-3, PaLM, Pile) use heuristics: simple classifier + a noisy threshold to select data similar to the target distribution.

But the selected data may lack diversity/be too diverse compared to the desired target dist. ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397380677681152))
** Our method, Data Selection with Importance Resampling (DSIR), uses importance resampling to select data that *matches* a target dist.

To make importance weight estimation tractable in the high-dimensional space of text, we work in a reduced feature space. ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397385111044097))
** Which feature space? N-grams! We find that KL reduction, a cheap data metric that measures how much data selection reduces KL to the target n-gram distribution over random sampling, predicts downstream acc very well (r=0.89).

Potential for powering new data-centric workflows! 

![](https://pbs.twimg.com/media/FoaaR2baEAAZXEe.jpg) ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397389607358472))
** This leads to our instantiation of DSIR, where we train 2 generative bag-of-ngrams models (one each for the raw and target data) to estimate importance weights. The n-grams are hashed onto a fixed number of virtual tokens (hashing trick) for simplicity and tractability. ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397395630366720))
** When selecting for formal/high-quality text (Wiki+books) to train general-domain LMs, DSIR selects data that contains qualitatively more formal text than random selection and heuristic filters. 

This results in 2‚Äì2.5% higher downstream acc on GLUE than these baselines. 

![](https://pbs.twimg.com/media/FoaXcTXakAEZQJX.png) ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397400005033984))
** When selecting for domain-specific data to train specialized LMs, DSIR gets comparable or better results to expert-curated data on average across 8 tasks from 4 diverse domains.

Shows the potential for automatic data selection to replace manual/bespoke processes! ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397406141214723))
** Does choice of pretraining data matter? We selected pretraining data for 8 downstream tasks and tried all pretrain->downstream pairs. Using the wrong pretraining data causes a 6% average drop in downstream acc, and drops acc by 30% in one case!

Choice of data matters a lot. 

![](https://pbs.twimg.com/media/FoaYfknaUAUOhX_.png) ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397410708807680))
** Generally, transfer across domains is very asymmetric. 

Pretraining on data selected for a ‚ÄúCS academic papers‚Äù target distribution usually has positive transfer to other domains, while data selected for a ‚Äúcustomer reviews‚Äù target distribution results in negative transfer... 

![](https://pbs.twimg.com/media/FoaYvLbaUAArqzG.jpg) ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397417847558144))
** N-gram features are great for importance resampling: cheap to compute, scalable, and overlap in n-gram distributions matters for downstream transfer.

But other pretraining data factors could matter for downstream acc. Designing the right features could improve data selection. ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397424914919425))
** Joint work with my wonderful collaborators and advisors @ShibaniSan @tengyuma @percyliang! ([View Tweet](https://twitter.com/sangmichaelxie/status/1623397429578981378))