:PROPERTIES:
:title: readwise/1_13 ğŸ§µğŸ’¡ Ever Wondered Ho...
:END:


* metadata
:PROPERTIES:
:author: [[EdenEmarco177 on Twitter]]
:full-title: "1/13 ğŸ§µğŸ’¡ Ever Wondered Ho..."
:category: [[tweets]]
:url: https://twitter.com/EdenEmarco177/status/1667555367314751488
:image-url: https://pbs.twimg.com/profile_images/1361898811031969795/z1fzrwB1.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
1/13 ğŸ§µğŸ’¡ Ever wondered how to handle token limitations of LLMs? Here's one strategy of the "map-reduce" technique implemented in @LangChainAI  ğŸ¦œğŸ”—
Let's deep dive!  @hwchase17 's your PR is under review againğŸ˜ https://t.co/grcDjKSE0v 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
2/13 MapReduce is not new. Famously introduced by @Google , it's a programming model that allows for the processing and generation of large data sets with a parallel, distributed algorithm. 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
3/13 In essence, it divides work into small parts that can be done simultaneously (the â€œmappingâ€) and then merge the intermediate results back to a one final result (â€œreducingâ€). 

![](https://pbs.twimg.com/media/FyRUPqJWYAU8T4l.png) 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
4/13 Letâ€™s take a real world example. You want your LLM to summarize a very very long document- Which obviously surpasses the LLMâ€™s token limit- so sending the entire document in the context will result in a â€œtoo many tokensâ€ error. 

![](https://pbs.twimg.com/media/FyRXIulXsAAmWWg.png) 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
5/13  So, how does @LangChainAI ğŸ¦œğŸ”— help with this? When dealing with a large piece of text,  @LangChainAI  can seamlessly break it down into manageable parts- 'chunks'. This forms our data set for the 'map' operation. 

![](https://pbs.twimg.com/media/FyRU-8MXoAAkQru.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
6/13 Each 'chunk' is then processed independently by an LLM, generating a separate summary (LLM call per chunk). This is our 'map' operation, creating a set of summaries. 

![](https://pbs.twimg.com/media/FyRVFwvWAAE6VYY.png) 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
7/13 Here is the prompt template from @LangChainAI  ğŸ¦œğŸ”—source code written by @hwchase17 

![](https://pbs.twimg.com/media/FyRVUSGWIAENtoz.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
8/13 After the 'map' step, all the independent summaries are combined using the 'reduce' step. This is another call to the LLM, asking it to synthesize the separate summaries into a final, unified one. 

![](https://pbs.twimg.com/media/FyRVcI8XgAEB9wg.png) 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
9/13 The cool thing about â€œmap-reduceâ€ is that the â€œmapâ€ steps can be done in parallelğŸ˜ greatly speeding up the process of large volumes of text. This makes it an excellent fit for distributed computing environmentsâ˜ï¸ 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
10/13  Keep in mind, however, that â€˜map-reduceâ€™ works best when the individual parts donâ€™t have much interdependence . This means that this method may be less effective if the text chunks need to be understood in context with each other. 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
11/13  Despite this, by cleverly applying the 'map-reduce' concept, @LangChainAI ğŸ¦œğŸ”— is able to efficiently generate meaningful summaries from large-scale texts, showcasing the power of combining distributed computing techniques with LLMs. 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
12/13 So how does this entire process look in code? It's actually pretty short since @LangChainAI ğŸ¦œğŸ”— does all the heavy lifting for us. 

![](https://pbs.twimg.com/media/FyRVz3uX0AA_k-Q.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-06-11]]
#+BEGIN_QUOTE
13/13 If you are interested in learning  @LangChainAI ğŸ¦œğŸ”— by building cool stuff-
Get me a virtual coffee and have acess my best-selling @udemy  course (read the reviews ğŸ˜)
https://t.co/rGxs9pMW0t

 Coupon code for exclusive twitter discount:
TWITTER9DCC71C67A9AA 
#+END_QUOTE\