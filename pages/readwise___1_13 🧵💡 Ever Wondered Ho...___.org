:PROPERTIES:
:title: readwise/1_13 🧵💡 Ever Wondered Ho...
:END:


* metadata
:PROPERTIES:
:author: [[EdenEmarco177 on Twitter]]
:full-title: "1/13 🧵💡 Ever Wondered Ho..."
:category: [[tweets]]
:url: https://twitter.com/EdenEmarco177/status/1667555367314751488
:image-url: https://pbs.twimg.com/profile_images/1361898811031969795/z1fzrwB1.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
1/13 🧵💡 Ever wondered how to handle token limitations of LLMs? Here's one strategy of the "map-reduce" technique implemented in @LangChainAI  🦜🔗
Let's deep dive!  @hwchase17 's your PR is under review again😎 https://t.co/grcDjKSE0v 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
2/13 MapReduce is not new. Famously introduced by @Google , it's a programming model that allows for the processing and generation of large data sets with a parallel, distributed algorithm. 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
3/13 In essence, it divides work into small parts that can be done simultaneously (the “mapping”) and then merge the intermediate results back to a one final result (“reducing”). 

![](https://pbs.twimg.com/media/FyRUPqJWYAU8T4l.png) 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
4/13 Let’s take a real world example. You want your LLM to summarize a very very long document- Which obviously surpasses the LLM’s token limit- so sending the entire document in the context will result in a “too many tokens” error. 

![](https://pbs.twimg.com/media/FyRXIulXsAAmWWg.png) 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
5/13  So, how does @LangChainAI 🦜🔗 help with this? When dealing with a large piece of text,  @LangChainAI  can seamlessly break it down into manageable parts- 'chunks'. This forms our data set for the 'map' operation. 

![](https://pbs.twimg.com/media/FyRU-8MXoAAkQru.jpg) 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
6/13 Each 'chunk' is then processed independently by an LLM, generating a separate summary (LLM call per chunk). This is our 'map' operation, creating a set of summaries. 

![](https://pbs.twimg.com/media/FyRVFwvWAAE6VYY.png) 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
7/13 Here is the prompt template from @LangChainAI  🦜🔗source code written by @hwchase17 

![](https://pbs.twimg.com/media/FyRVUSGWIAENtoz.jpg) 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
8/13 After the 'map' step, all the independent summaries are combined using the 'reduce' step. This is another call to the LLM, asking it to synthesize the separate summaries into a final, unified one. 

![](https://pbs.twimg.com/media/FyRVcI8XgAEB9wg.png) 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
9/13 The cool thing about “map-reduce” is that the “map” steps can be done in parallel😎 greatly speeding up the process of large volumes of text. This makes it an excellent fit for distributed computing environments☁️ 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
10/13  Keep in mind, however, that ‘map-reduce’ works best when the individual parts don’t have much interdependence . This means that this method may be less effective if the text chunks need to be understood in context with each other. 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
11/13  Despite this, by cleverly applying the 'map-reduce' concept, @LangChainAI 🦜🔗 is able to efficiently generate meaningful summaries from large-scale texts, showcasing the power of combining distributed computing techniques with LLMs. 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
12/13 So how does this entire process look in code? It's actually pretty short since @LangChainAI 🦜🔗 does all the heavy lifting for us. 

![](https://pbs.twimg.com/media/FyRVz3uX0AA_k-Q.jpg) 
#+END_QUOTE\
** 📌 [[2023-06-11]]
#+BEGIN_QUOTE
13/13 If you are interested in learning  @LangChainAI 🦜🔗 by building cool stuff-
Get me a virtual coffee and have acess my best-selling @udemy  course (read the reviews 😎)
https://t.co/rGxs9pMW0t

 Coupon code for exclusive twitter discount:
TWITTER9DCC71C67A9AA 
#+END_QUOTE\