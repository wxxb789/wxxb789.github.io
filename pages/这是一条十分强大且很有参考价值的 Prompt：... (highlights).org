:PROPERTIES:
:title: 这是一条十分强大且很有参考价值的 Prompt：... (highlights)
:END:

:PROPERTIES:
:author: [[Barret_China on Twitter]]
:full-title: "这是一条十分强大且很有参考价值的 Prompt：..."
:category: [[tweets]]
:url: https://twitter.com/Barret_China/status/1714820412859969594
:END:

* Highlights first synced by [[Readwise]] [[2023-10-19]]
** 📌
** #+BEGIN_QUOTE
** 这是一条十分强大且很有参考价值的 Prompt：https://t.co/51axmyHjfC，跟它进行第一轮对话后，它会帮助你召唤出适合该任务的专家，在与专家进行第二轮对话后，ChatGPT 会完全明白你的目标，然后给出详细的分解和实施过程。

为什么这条 Prompt 表现效果会非常好呢？它的整个过程分成了两个阶段，第一阶段是明确要做的事情，把任务的轮廓给描绘清楚，第二阶段则是深入到细节，收集任务全貌所需要的信息。

之前也提到过（https://t.co/tAGYp7mVRI），GPT 跟人类的思考方式是完全不一样的，它的每一个字符（Token）生成都是通过概率推导的方式获得的，而且生成每个字符所消耗的计算资源一样。推导的过程中会出现高质量和低质量的结果，它自己是知道质量高低的，但是默认的情况下，它就是要推导出所有的可能性。

第一阶段的操作，很大程度减少了大模型在推理过程中的分支数量，对于不符合目标的推理权重会大大降低。

而第二阶段其实是在做拆解任务的工作，首先召唤出来的专家会根据目标生成一份执行计划，然后让你参与到对话过程，不断完善这份计划。这有点像之前（https://t.co/6Y38ciBnFF）提到的 CoT + Least to Most prompting + Multi-Persona Self-Collaboration 的结合。

两个阶段的对话内容将会成为 ChatGPT 生成最终内容的重要参考语料，后续的推理权重会更大程度偏向于任务目标和执行计划。因此这个 Prompt 可以说，写的非常非常好，充分利用了大模型底层的运作机制。  ([View Tweet](https://twitter.com/Barret_China/status/1714820412859969594))
** #+END_QUOTE