#+public: true
#+tags: gpt, llm, openai, ai,

* resources
** [[links]]
*** [[https://cookbook.openai.com/][OpenAI Cookbook]]
openai doc
*** [[https://www.youtube.com/@OpenAI][OpenAI YouTube]]
openai conference
*** [[https://openai.com/blog][Blog (openai.com)]]
openai blog
*** [[https://www.deeplearning.ai/short-courses/][Short Courses | Learn Generative AI from DeepLearning.AI]]
deeplearning.ai 短课程
*** [[https://space.bilibili.com/243505935/channel/seriesdetail?sid=3248149][智能工程搬砖师的个人空间]]
deeplearning.ai 短课程
*** [[https://microsoft.github.io/AI-For-Beginners/][AI for Beginners (microsoft.github.io)]]
Microsoft AI programming tutorial
** [[Book]]
*** [[https://book.douban.com/subject/36449803/][这就是ChatGPT (豆瓣) (douban.com)]]
作者 Stephen Wolfram，计算机科学、数学和理论物理学家，当今科学和技术领域重要的革新者之一。
* concepts
** [[prompt]]
*** 描述
prompt 是机器学习模型中的输入文本，用于指导模型生成特定的输出。在GPT和其他自然语言处理模型中，prompt起到了定义任务和提供上下文的作用。
*** 作用
prompt 的主要作用是为模型提供方向性的信息，帮助模型了解用户的意图，并据此产生合适的响应。它的设计直接影响模型的表现和输出的相关性。
*** 例子
例如，在ChatGPT中，用户提问“机器学习是什么？”就是一个prompt，模型根据这个输入生成关于机器学习的解释。
** [[RLHF]]
*** 描述
RLHF (Reinforcement Learning from Human Feedback) 是一种结合了强化学习和人类反馈的训练方法。它利用人类提供的反馈来指导模型的行为，增强模型在特定任务上的表现。
*** 作用
RLHF的作用是提高模型在复杂任务上的性能，特别是在那些难以通过传统监督学习方法获得高质量标签的情况下。它使模型能够更好地理解和满足用户的需求。
*** 例子
在ChatGPT中使用RLHF来优化对话模型，通过人类对话者的反馈调整模型的回答方式。
** [[InstructGPT]]
*** 描述
InstructGPT是OpenAI开发的GPT模型的一个变体，专门调整以更好地遵循用户的指令和意图。
*** 作用
它的主要作用是提供更准确、更符合用户指令的输出。相比传统的GPT模型，InstructGPT在理解和执行复杂指令方面表现更好。
*** 例子
当用户请求“列出健康饮食的五个建议”时，InstructGPT会根据指令生成具体且相关的建议列表。
** [[transformer]]
*** 描述
transformer是一种深度学习模型架构，特别适用于处理序列数据，如文本。它依靠自注意力机制来捕捉序列中的关系。
*** 作用
transformer的主要作用是提高自然语言处理任务的效率和准确性，特别是在长文本和复杂语境下。它是现代NLP模型的核心组件。
*** 例子
GPT系列模型就是基于transformer架构，能有效处理和生成自然语言文本。
** [[function calling]]
*** 描述
function calling是指在程序中调用一个函数，以执行特定的操作或计算。在编程中，函数是一段可以重复使用的代码块。
*** 作用
function calling的作用是提高代码的模块化和复用性，简化复杂程序的开发和维护。
*** 例子
在Python中，调用`print("Hello World")`会执行print函数，输出字符串到控制台。
** [[RAG]] Retrieval-Augmented Generation
*** 描述
RAG是一种结合了检索（retrieval）和生成（generation）的NLP方法。它先从大量文本中检索相关信息，再基于这些信息生成响应。
*** 作用
RAG的作用是提升模型在生成准确、信息丰富的文本方面的能力，特别是在需要外部知识的情况下。
*** 例子
在问答系统中，RAG可以先检索相关文章或资料，然后基于这些资料生成准确的答案。
** [[Embedding]]
*** 描述
Embedding是一种将词汇、句子或其他类型的数据映射为数值向量的技术。在自然语言处理中，它把词转换为向量，以便计算机处理。
*** 作用
Embedding的主要作用是将文本数据转换成机器学习模型可以处理的格式，有助于提高模型对语言数据的理解。
*** 例子
在文本分类任务中，使用word embedding将词转换为向量，然后用这些向量训练分类模型。
** [[Token]]
*** 描述
Token在自然语言处理中指的是文本中的最小单位，如词、字符或子词。Tokenization是将文本分割成token的过程。
*** 作用
Token的作用是将文本分解成更易于处理的小单元，为后续的处理步骤（如embedding或模型训练）奠定基础。
*** 例子
在处理句子“The cat sat on the mat”时，将其分割为tokens：“The”, “cat”, “sat”, “on”, “the”, “mat”。
** [[Attention Mechanism]]
*** 描述
Attention Mechanism是一种让模型在处理序列数据时能够聚焦于重要部分的技术。Self-attention是一种特殊形式，让模型在处理一个序列时考虑序列内部的关系。
*** 作用
它们的作用是提高模型在处理长序列时的性能，特别是在理解上下文和捕捉长距离依赖方面。
*** 例子
在翻译句子时，attention mechanism可以帮助模型专注于当前译文部分对应的原文部分。
** [[Self-Attention Mechanism]]
*** 描述
Self-Attention机制是一种在深度学习和自然语言处理领域中常见的技术。它允许模型在处理序列（如文本或时间序列数据）时关注序列中的不同部分。这种机制通过计算序列中每个元素与其他所有元素之间的关系，来确定每个元素的重要性。Self-Attention通常通过一组权重来实现，这些权重决定了序列中不同元素对当前元素的影响程度。
*** 作用
Self-Attention机制的主要作用是提高模型对序列数据的理解能力。它使模型能够捕捉到序列中长距离依赖的关系，这在传统的序列处理模型中是难以实现的。例如，在文本处理中，Self-Attention可以帮助模型理解距离较远的单词之间的关系，从而提高语言理解和生成的准确性。此外，Self-Attention还提高了模型的并行处理能力，因为它可以同时处理序列中的多个元素。
*** 例子
一个典型的例子是在Transformer模型中使用的Self-Attention机制。在处理文本时，Transformer利用Self-Attention来理解句子中各个单词之间的关系。例如，当模型处理句子“The cat sat on the mat”时，Self-Attention机制能够识别“cat”和“sat”之间的主谓关系，以及“on”和“the mat”之间的介词关系。这种理解加强了模型对整个句子结构的把握，从而提高了翻译、摘要和文本生成等任务的性能。
** [[few-shot]]
*** 描述
Few-shot learning 是一种机器学习方法，它使模型能够仅通过少量样本（即“shots”）学习新任务或概念。这与传统的机器学习方法相反，后者通常需要大量的数据才能训练模型。
*** 作用
Few-shot learning 的主要作用是提高模型在数据稀缺环境下的泛化能力。这对于那些难以收集或标注大量数据的场景尤为重要，比如医学影像诊断或稀有语言的语言处理。
*** 例子
在医学影像分析中，使用 few-shot learning 来训练一个能够识别罕见病变的模型，只需要少量标注的病变图像。
** [[zero-shot]]
*** 描述
Zero-shot learning 是一种让模型在没有直接经验的情况下理解或执行任务的能力。这意味着模型没有被直接训练去处理特定任务，但它能够利用其已有知识来解决新问题。
*** 作用
Zero-shot learning 使得模型能够处理那些在训练期间未曾见过的数据或任务，增强了模型的灵活性和适应性。这在处理多样化和不断变化的真实世界问题中尤其重要。
*** 例子
一个使用 zero-shot learning 的语言模型可以在没有直接训练的情况下，理解并回答关于全新主题的问题，如最新的科技发明或未曾接触过的文化。
** [[fine-tuning]]
*** 描述
Fine-tuning 是在预训练模型的基础上，通过少量特定任务数据进行再训练的过程。这种方法利用了预训练模型的通用知识，并通过额外的训练使其更适应特定任务。
*** 作用
Fine-tuning 的作用是提升模型在特定任务上的表现，同时减少了从头开始训练模型所需的大量数据和计算资源。
*** 例子
在自然语言处理领域，可以对 GPT-3 进行 fine-tuning，以使其更好地执行特定的文本生成任务，如撰写特定领域的文章。
** [[pre-training]]
*** 描述
Pre-training 是指在大量数据集上训练一个模型，以使其学习广泛的特征和模式。这个过程通常是无监督或半监督的，目的是捕捉通用的知识。
*** 作用
Pre-training 的主要作用是创建一个具有强大泛化能力的基础模型，这个模型可以被进一步 fine-tune 用于各种特定任务。
*** 例子
BERT（Bidirectional Encoder Representations from Transformers）是一个典型的预训练模型，它在大量文本上进行预训练，以理解语言的深层次结构。
** [[agent]], multi-agent
*** 描述
在 AI 和机器学习领域，agent 指的是能够感知环境并作出决策的实体。在 multi-agent 系统中，多个 agent 相互作用，共同解决问题或完成任务。
*** 作用
Agent 的作用是在其所处环境中执行任务，如学习、决策和行动。在 multi-agent 系统中，这些 agent 通过合作或竞争来实现更复杂的目标。
*** 例子
在自动驾驶汽车中，每辆车可以被视为一个 agent，它们通过感知环境并做出决策来安全驾驶。在 multi-agent 系统中，这些车辆还可以相互通信，以优化交通流量和安全性。
** [[langchain]]
*** 描述
Langchain 是一个基于语言模型的框架，用于构建和组合多个 AI 组件，以创建更复杂的应用。
*** 作用
Langchain 的主要作用是简化在多个 AI 组件之间建立语言理解和生成的过程，使得开发者可以更容易地构建复杂的语言处理应用。
*** 例子
使用 Langchain，开发者可以组合一个聊天机器人、一个问答系统和一个文本摘要生成器，创建一个综合的客户服务应用。
** [[vector db]]
*** 描述
Vector database（vector db）是一种存储和检索向量数据（如嵌入向量）的数据库系统。Vector search 是在这些数据库中进行高效搜索的过程。
*** 作用
Vector db 和 vector search 的主要作用是使得基于向量的数据检索更加快速和准确，这对于处理大量复杂数据（如文本、图像和音频）至关重要。
*** 例子
在文本搜索应用中，可以使用 vector search 在 vector db 中快速找到与查询语句最相关的文档或文章。
** [[vector search]]
*** 描述
Vector search，或向量搜索，是一种基于机器学习技术的搜索方法。它通过将文本、图像等数据转换成数学上的向量表示，再在这些向量之间进行搜索。这种方法可以捕捉到数据的深层次语义信息，而不仅仅是表面的关键词匹配。
*** 作用
Vector search 的主要作用是提高搜索效率和准确性。传统的基于关键词的搜索方法在处理复杂的、含有丰富语义信息的查询时常常力不从心。向量搜索能够更好地理解和匹配用户的查询意图，从而提供更为相关和精准的搜索结果。
*** 例子
一个典型的应用例子是在在线购物平台上，用户通过上传一张衣服的照片来搜索类似款式的产品。系统将这张图片转换成向量，然后在商品数据库的向量表示中进行匹配，找到视觉上相似的商品。
* additional concepts
** [[Machine Learning]]
*** 描述
机器学习 (Machine Learning, ML) 是人工智能 (AI) 的一个分支，它使计算机能够通过数据学习并做出决策或预测。机器学习算法使用统计技术对大量数据进行模式识别和分析，从而使计算机能够学习而无需进行明确编程。
*** 作用
机器学习的主要作用是分析和解释数据，用以预测和决策。它在各种领域中应用广泛，包括金融市场分析、医疗诊断、图像和语音识别、推荐系统等。
*** 例子
一个典型的例子是Netflix推荐算法，它通过分析用户的观影历史和偏好，推荐可能感兴趣的电影和电视节目。
** [[Deep Learning]]
*** 描述
深度学习 (Deep Learning, DL) 是机器学习的一个子领域，它通过模拟人脑中的神经网络结构来学习数据的表示和特征。深度学习使用多层的神经网络（Deep Neural Networks）来处理和学习复杂的数据。
*** 作用
深度学习在处理大规模和复杂的数据方面特别有效，例如图像识别、语音处理和自然语言处理。它能够识别和解释数据中的复杂模式和结构。
*** 例子
谷歌的AlphaGo是一个典型例子，它使用深度学习算法在围棋游戏中击败了世界冠军。
** [[BERT]]
*** 描述
BERT (Bidirectional Encoder Representations from Transformers) 是一种预训练的深度学习模型，专门用于自然语言处理 (NLP)。它通过从文本中的前后两个方向同时学习上下文信息，来更好地理解语言的含义。
*** 作用
BERT在文本分析、情感分析、问答系统和语言理解任务中表现优异。它能够更准确地理解和处理人类语言的复杂性和微妙之处。
*** 例子
Google Search使用BERT来理解搜索查询的意图，提供更相关和准确的搜索结果。
** [[Seq2Seq]] Sequence to Sequence Models
*** 描述
序列到序列 (Sequence to Sequence, Seq2Seq) 模型是一种处理序列数据的深度学习框架，通常包括两个主要部分：编码器和解码器。编码器处理输入序列，解码器生成输出序列。
*** 作用
Seq2Seq模型在机器翻译、语音识别、文本摘要等领域中非常有效。它们可以将一个序列转换为另一个序列，同时保持内容的含义和上下文关系。
*** 例子
Google翻译使用Seq2Seq模型将一种语言的文本翻译成另一种语言。
** [[Supervised Learning]]
*** 描述
监督学习 (Supervised Learning) 是机器学习中一种基于标记数据进行学习的方法。在这种方法中，算法被训练在给定的输入和输出样本上，目的是学习一个映射函数，从而对新的输入进行预测。
*** 作用
监督学习广泛应用于分类和回归问题，如邮件垃圾过滤、股票价格预测等。它通过训练数据集学习如何将输入映射到正确的输出。
*** 例子
信用评分模型根据个人的财务历史数据预测其信用风险，这是监督学习的一个应用。
** [[Unsupervised Learning]]
*** 描述
无监督学习 (Unsupervised Learning) 是指在没有标记输出的情况下，从数据中学习模式和结构的机器学习方法。这种方法专注于探索数据内在结构和关联。
*** 作用
无监督学习主要用于聚类分析、异常检测、关联规则学习等。它帮助发现数据中的隐藏模式和关系。
*** 例子
客户细分是无监督学习的一个典型例子，企业通过分析客户数据，将客户分为不同的群体，以实现更有效的市场定位。
** [[Reinforcement Learning]]
*** 描述
强化学习 (Reinforcement Learning, RL) 是一种机器学习范式，其中学习代理通过与环境交互来学习如何在特定任务上表现最佳。代理根据其行为获得的奖励或惩罚进行学习。
*** 作用
强化学习在自动驾驶汽车、机器人导航、游戏玩法等领域中得到了应用。它使代理能够自主学习最优策略，以在复杂环境中实现目标。
*** 例子
DeepMind的AlphaGo通过强化学习训练，学会了高水平的围棋策略。
** [[Discriminative Models]]
*** 描述
判别模型 (Discriminative Models) 是一种用于区分不同类别数据的机器学习模型。它们直接学习从输入到输出类别的映射，而不是生成数据的整体分布。
*** 作用
判别模型在分类任务中非常有效，如图像识别、语音识别等。它们专注于区分不同类别的特征。
*** 例子
图像识别系统，如用于人脸识别的模型，就是使用判别模型来区分不同人脸的例子。
** [[Backpropagation]]
*** 描述
反向传播 (Backpropagation) 是一种用于训练神经网络的算法。它通过计算损失函数（预测错误的量度）相对于网络权重的梯度，来更新网络的权重，从而最小化损失。
*** 作用
反向传播是深度学习中最核心的算法之一。它使得神经网络能够通过迭代学习来改进其性能，逐渐减少预测误差。
*** 例子
在手写数字识别中，反向传播用于训练神经网络，使其能够更准确地识别不同的数字。
** [[Activation Function]] 激活函数
*** 描述
激活函数在人工神经网络中起着至关重要的角色，它们决定着一个神经元是否应该被激活，即输出的信号是否足以传递到下一个层次。这些函数引入非线性因素，使得网络能处理复杂的数据如图像、声音等。
*** 作用
通过非线性转换，激活函数帮助网络学习和表示各种复杂的模式。它们也使得反向传播算法能够有效工作，因为这些算法依赖于导数，而非线性函数提供了可微性。
*** 例子
常见的激活函数包括 [[Sigmoid]]、[[ReLU]]（Rectified Linear Unit）、[[Tanh]] 等。例如，在二分类问题中，Sigmoid函数可以将输出转化为概率。
** [[Hyperparameters Tunning]] 超参数调优
:PROPERTIES:
:id: 656c9899-d47e-424c-84c7-e545f50850bc
:END:
*** 描述
超参数调优是机器学习中的一个重要过程，涉及到选择和优化那些在学习过程开始前设定的参数（即超参数）。这些参数控制着学习算法的行为，但并不会在学习过程中自动更新。
*** 作用
适当的超参数设置可以显著提升模型的性能。超参数调优通常通过尝试一系列的参数组合来完成，目的是找到最优化模型性能的组合。
*** 例子
例如，在神经网络中，超参数可能包括学习率、批处理大小、训练轮次（epoch）等。使用网格搜索（Grid Search）或随机搜索（Random Search）来找到最佳组合是一种常见做法。
** [[Loss Function]] 损失函数
*** 描述
损失函数是一个衡量模型预测值与真实值差异的函数。在优化算法中，损失函数的值是被最小化的目标，它反映了模型的预测精度。
*** 作用
损失函数的主要作用是引导模型学习，通过最小化损失函数来调整模型参数，使模型的预测更接近真实标签。
*** 例子
常见的损失函数包括均方误差（MSE）用于回归问题，交叉熵损失（Cross-Entropy Loss）用于分类问题。
** [[Gradient Descent]] 梯度下降
*** 描述
梯度下降是一种优化算法，用于最小化损失函数，通过迭代的方式更新模型的参数。它通过计算损失函数关于参数的梯度来确定更新的方向和步长。
*** 作用
梯度下降帮助模型找到损失函数的最小值，即找到能最佳拟合数据的参数。它是许多机器学习和深度学习算法中的核心部分。
*** 例子
在神经网络训练中，梯度下降通过不断更新权重和偏差来减少预测误差。
** [[Batch]], [[Epoch]], and [[Iteration]] 批次、轮次和迭代
*** 描述
这些术语是训练神经网络时经常使用的。批次（Batch）指的是用于一次迭代训练的样本集合；轮次（Epoch）是整个训练数据集被循环通过网络的次数；迭代（Iteration）是完成一个批次所需的前向传递和后向传递的总次数。
*** 作用
理解这些概念有助于更好地配置训练过程，以及理解训练过程中的学习动态。
*** 例子
假设有一个包含1000个样本的数据集，如果设置批次大小为100，则每个轮次将有10个迭代。
** [[CNN]] 卷积神经网络 Convolutional Neural Networks
*** 描述
卷积神经网络是一种专门用于处理具有类似网格结构的数据的深度学习网络（如图像）。CNN通过卷积层来提取特征，然后使用全连接层进行分类或其他任务。
*** 作用
CNN在特征提取方面非常有效，它能够捕捉到输入数据的空间和时间依赖性，使其在图像和视频识别、图像分类等领域表现优异。
*** 例子
在图像识别任务中，CNN可以识别和分类图像中的对象，如识别猫和狗。
** [[RNN]]循环神经网络 Recurrent Neural Networks
*** 描述
循环神经网络是一种专门用于处理序列数据的神经网络，如文本、声音或时间序列数据。RNN的特点是它的输出不仅取决于当前的输入，还取决于之前的输入。
*** 作用
RNN通过其循环结构使得模型能够存储之前的信息，并利用这些信息影响当前和未来的输出，使其特别适用于语言模型和其他序列数据任务。
*** 例子
在自然语言处理中，RNN可以用于文本生成、机器翻译等任务。
** [[Regularization]]
*** 描述
Regularization 是一种减少模型过拟合（overfitting）的技术，通过在训练过程中添加一个惩罚项来实现。这个惩罚项通常是模型权重的函数，其目的是限制模型的复杂度，从而使模型在训练数据上表现良好的同时，也能在未见数据上保持泛化能力。
*** 作用
Regularization 的主要作用是提高模型的泛化能力，避免过拟合。这是通过降低模型复杂度和鼓励更平滑的模型行为来实现的。常见的 Regularization 方法包括 L1 和 L2 正则化，它们通过对模型权重施加惩罚，促使模型偏向于更小、更分散的权重。
*** 例子
在线性回归模型中使用 L2 正则化（也称为岭回归）：通过添加权重平方和的惩罚项到损失函数中，可以限制模型权重的大小，从而减少模型对训练数据中的噪声或非代表性特征的过度拟合。
** [[Overfitting]] and [[Underfitting]]
*** 描述
Overfitting 指的是模型在训练数据上表现得太好，以至于捕捉到了数据中的噪声和细节，而不是潜在的数据分布。Underfitting 则是指模型在训练数据上的表现不足，无法捕捉数据中的基本结构。
*** 作用
理解 overfitting 和 underfitting 对于评估和改进机器学习模型的性能至关重要。通过识别这两种情况，可以采取适当的措施（如调整模型复杂度、增加训练数据、改变模型架构等）来改善模型的泛化能力。
*** 例子
一个高度复杂的深度神经网络在一个小数据集上可能会出现 overfitting，表现为训练误差远小于验证误差。而一个过于简单的模型（如线性回归）可能在复杂数据集（如非线性数据）上 underfitting，表现为训练误差和验证误差都很高。
** Long Short Term Memory (LSTM)
*** 描述
LSTM 是一种特殊的循环神经网络（RNN）架构，专门设计来解决传统 RNN 在处理长序列数据时的梯度消失和爆炸问题。LSTM 通过引入门控机制（包括输入门、遗忘门和输出门）来调控信息的流动，从而有效地保留长期依赖信息。
*** 作用
LSTM 能够有效处理和记忆长时间跨度的信息，非常适合于时间序列分析、自然语言处理等需要处理长序列数据的应用场景。它通过门控机制在保留长期信息和避免梯度问题方面表现出色。
*** 例子
在自然语言处理中，LSTM 可用于构建语言模型，如基于文本的序列生成（如聊天机器人、文本摘要等），它能够记住较长文本序列中的上下文信息，从而生成连贯和相关的文本。
** [[GAN]] Generative Adversarial Network
*** 描述
GAN 是一种深度学习模型，由两部分组成：生成器（Generator）和判别器（Discriminator）。生成器负责生成看起来像真实数据的假数据，而判别器的任务是区分生成的假数据和真实数据。
*** 作用
GAN 在生成逼真的数据方面表现出色，广泛应用于图像合成、风格迁移、数据增强等领域。通过不断的对抗过程，GAN 能够学习到数据的深层特征和分布，生成高质量的数据。
*** 例子
在图像处理中，GAN 可用于生成新的、看起来真实的人脸图像或风景图片。例如，使用 GAN 生成艺术风格迁移的图片，其中生成器学习如何将一种艺术风格应用到图像上，而判别器则学习区分真实的艺术作品和生成器产生的作品。
** [[Transfer Learning]]
*** 描述
Transfer Learning 是一种机器学习技术，涉及将在一个任务上学到的知识应用到不同但相关的任务上。这通常通过使用在大型数据集上预训练的模型作为起点，然后对其进行微调以适应新任务来实现。
*** 作用
Transfer Learning 允许模型利用在其他任务上获得的知识，提高学习效率和性能，尤其是在数据受限的情况下。这使得可以在较少数据上构建强大的模型，加速开发过程，并提高模型在新任务上的表现。
*** 例子
在图像识别领域，可以使用在 ImageNet 数据集上预训练的深度学习模型（如 ResNet）来识别特定类别的对象。通过在特定任务的较小数据集上微调这些模型，可以迅速获得高性能的定制模型。
** [[Data Augmentation]]
*** 描述
Data Augmentation 是一种提高模型泛化能力的技术，通过对训练数据进行修改或增加变化来扩大训练集。常见的方法包括翻转、旋转、缩放、变换颜色等，这些操作可以增加数据的多样性，减少模型对特定数据特征的依赖。
*** 作用
Data Augmentation 主要用于扩大训练数据集的规模和多样性，从而提高模型的泛化能力。这在数据受限的情况下尤其有用，可以有效减少 overfitting。
*** 例子
在图像分类任务中，通过对训练图像应用随机翻转、旋转和颜色调整，可以生成更多变化的图像。这有助于训练出能够在不同条件下准确识别对象的模型。
** [[One-Hot Encoding]]
*** 描述
One-Hot Encoding 是一种在机器学习和深度学习中常用的数据预处理方法，用于将类别型数据转换为数值型数据。在这种编码中，每个类别都被表示为一个仅在一个位置为 1，其余位置为 0 的向量。这种表示方法可以使算法更容易处理类别数据。
*** 作用
One-hot encoding 的主要作用是将文本数据或类别数据转换为机器学习模型可以处理的格式。它消除了类别之间的数值关联，确保算法不会错误地解释类别之间的顺序或距离。
*** 例子
假设有一个数据集，其中包含三个类别：“苹果”，“香蕉”，“橙子”。使用 one-hot encoding，这些类别可以被编码为：苹果 = [1, 0, 0]，香蕉 = [0, 1, 0]，橙子 = [0, 0, 1]。
** [[Word2Vec]]
*** 描述
Word2Vec 是一种用于自然语言处理的模型，由 Google 的研究团队开发。它能够学习词汇的向量表示，其中每个单词被转换为一个固定大小的向量。这些向量能够捕捉词汇之间的语义关系和语境信息。
*** 作用
Word2Vec 的主要作用是提供一种高效的方式来表示单词，使得相关或相似的单词在向量空间中彼此接近。这种表示支持各种 NLP 任务，如文本分类、情感分析、机器翻译等。
*** 例子
例如，使用 Word2Vec 训练模型后，单词“王”和“皇后”可能具有相似的向量表示，因为它们在语义上相关且在文本中常常出现在相似的上下文中。
** [[Skip-gram]] 和 [[CBOW]]
*** 描述
Skip-gram 是一种用于生成词嵌入的模型。它通过使用一个目标词来预测其上下文中的单词。相比之下，CBOW（Continuous Bag of Words）模型预测一个目标词基于其上下文中的词汇。
*** 作用
Skip-gram 用于捕捉词汇之间的相似性和上下文关系，常用于自然语言处理中。CBOW 则更快且对频繁词效果更好，同样用于生成词嵌入。
*** 例子
在处理句子 "The quick brown fox" 时，Skip-gram 以 'quick' 为目标词，可能预测 'The', 'brown'；CBOW 则以 'The', 'brown' 作为输入，预测 'quick'。
** [[Bias]] and [[Variance]]
*** 描述
Bias 是预测误差的一部分，源于模型过于简单无法捕捉到数据的所有特征。Variance 是由于模型过于复杂，对训练数据的小波动过于敏感而产生的误差。
*** 作用
理解 Bias 和 Variance 对于改善模型性能至关重要。低 Bias 通常意味着高精度，而低 Variance 则提高模型对新数据的泛化能力。
*** 例子
高 Bias 的模型可能在复杂数据集上表现不佳（如线性模型应用于非线性数据），而高 Variance 的模型可能在训练集上表现良好但在测试集上表现差（如过拟合现象）。
** [[Cross-Validation]]
*** 描述
Cross-Validation 是一种评估模型泛化性能的技术，通过将数据分割成多个小子集，分别作为训练和验证用途，重复多次评估模型。
*** 作用
它帮助检测模型在未见数据上的表现，减少过拟合的风险，提高模型的泛化能力。
*** 例子
在 K-fold Cross-Validation 中，数据集被分成 K 个子集，每次用 K-1 个子集训练模型，剩下的一个用于验证。
** [[Precision]], [[Recall]], and [[F1 Score]]
*** 描述
Precision 是预测为正类的样本中真正正类的比例。Recall（召回率）是所有真正正类中被正确预测为正类的比例。F1 Score 是 Precision 和 Recall 的调和平均值。
*** 作用
这些指标帮助评估分类模型的性能，尤其在类别不平衡的情况下。
*** 例子
在医学检测中，高 Precision 意味着较少的假阳性，而高 Recall 表示较少的假阴性。F1 Score 提供了两者之间的平衡。
** [[Data Normalization]] and [[Standardization]]
*** 描述
Data Normalization 是将数据调整到一定范围（如 0 到 1）的过程。Standardization 是将数据调整为均值为 0，标准差为 1 的过程。
*** 作用
这些技术能够改善训练过程，加快收敛速度，提高模型性能。
*** 例子
在神经网络训练中，对输入特征进行 Standardization 可以帮助模型更快地学习。
** [[PCA]] Principal Component Analysis
*** 描述
PCA 是一种用于数据降维的技术，通过找到数据中的主要成分和去除噪声。
*** 作用
PCA 用于数据预处理、加快训练速度、数据可视化等，同时减少存储和计算成本。
*** 例子
在面部识别技术中，PCA 可用于降低图片的维度，提取重要特征。
** [[t-SNE]]
*** 描述
t-SNE（t-distributed Stochastic Neighbor Embedding）是一种用于数据可视化的机器学习算法，特别适用于高维数据的降维和可视化。
*** 作用
它通过保持高维空间中的相似性来映射数据到低维空间，使得类似的数据点在新空间中靠近。
*** 例子
t-SNE 常用于生物信息学中，如用于基因表达数据的可视化。
** [[Autoencoder]]
*** 描述
Autoencoder 是一种神经网络，用于学习数据的有效表示（编码），通常用于降维、特征学习。
*** 作用
它们通过学习重建输入数据，捕获数据中的关键特征。
*** 例子
在图像处理中，Autoencoder 可用于噪声去除，通过学习重建没有噪声的原始图像。
** [[AutoML]]
*** 描述
AutoML（Automated Machine Learning）指的是自动化机器学习流程的技术，包括特征选择、模型选择、调参等。
*** 作用
AutoML 使得非专业人士也能够高效使用机器学习，减少了对专业知识的依赖。
*** 例子
AutoML 工具可以自动选择最适合特定数据集的模型和参数，如 Google 的 Cloud AutoML。
* image
** [[vector search]]
*** 描述
Vector search，或向量搜索，是一种基于机器学习技术的搜索方法。它通过将文本、图像等数据转换成数学上的向量表示，再在这些向量之间进行搜索。这种方法可以捕捉到数据的深层次语义信息，而不仅仅是表面的关键词匹配。
*** 作用
Vector search 的主要作用是提高搜索效率和准确性。传统的基于关键词的搜索方法在处理复杂的、含有丰富语义信息的查询时常常力不从心。向量搜索能够更好地理解和匹配用户的查询意图，从而提供更为相关和精准的搜索结果。
*** 例子
一个典型的应用例子是在在线购物平台上，用户通过上传一张衣服的照片来搜索类似款式的产品。系统将这张图片转换成向量，然后在商品数据库的向量表示中进行匹配，找到视觉上相似的商品。
** [[DALL-E]]
*** 描述
DALL-E 是由 OpenAI 开发的一种人工智能程序，它能够根据用户的文字描述生成相应的图片。DALL-E 使用了深度学习模型，特别是变分自编码器（VAE）和 GPT 架构的结合，来理解复杂的语言指令并创造出新颖的图像内容。
*** 作用
DALL-E 的主要作用是在创意艺术、设计和视觉内容创作中提供辅助。它能够根据抽象或具体的描述生成图片，为艺术家、设计师或内容创作者提供灵感，或者直接作为创作工具使用。
*** 例子
例如，一个用户可能输入“在夕阳下，一只穿着太空服的猫站在火星表面”，DALL-E 就能生成对应这一描述的独特图像。
** [[stable diffusion]]
*** 描述
Stable Diffusion 是一种基于深度学习的图像生成技术。它使用了称为扩散模型的特殊神经网络，通过逐渐引导随机噪声数据转变为具体图像的方式来生成图片。这种方法能够生成高质量、逼真的图像。
*** 作用
Stable Diffusion 的作用主要体现在图像生成和数据增强方面。它能够根据简短的描述生成高度逼真的图像，也可以用于改进现有的图像，或生成全新的视觉内容。这对于图像编辑、艺术创作和娱乐产业等领域具有重要意义。
*** 例子
一个应用实例是在电影制作中，通过输入特定的场景描述，如“古代城堡在黎明时的景象”，Stable Diffusion 可以生成相应的场景图像，用于视觉效果的参考或直接应用。
** [[midjourney]]
*** 描述
Midjourney 是一个基于人工智能的创意探索工具，它结合了自然语言处理和图像生成技术。用户可以通过输入文字描述来引导 Midjourney 生成相应的视觉内容，这个过程类似于一场“中途旅程”，旨在激发创造性思考和视觉探索。
*** 作用
Midjourney 的主要作用是作为一种创意和灵感的催化剂，帮助艺术家、设计师和内容创作者探索新的视觉可能性。它可以迅速根据用户的描述生成图像，从而在设计和艺术创作过程中提供灵感和辅助。
*** 例子
例如，一个设计师在设计一个新的产品包装时，可以使用 Midjourney 输入“未来主义风格的可持续产品包装设计”，系统便会生成一系列创新的设计概念图供设计师参考。