:PROPERTIES:
:title: Extraction üìö‚û°Ô∏èüóíÔ∏è Gettin... (highlights)
:END:

:PROPERTIES:
:author: [[RLanceMartin on Twitter]]
:full-title: "Extraction üìö‚û°Ô∏èüóíÔ∏è Gettin..."
:category: [[tweets]]
:url: https://twitter.com/RLanceMartin/status/1687883380631932928
:END:

* Highlights first synced by [[Readwise]] [[2023-08-06]]
** üìå
** #+BEGIN_QUOTE
** Extraction üìö‚û°Ô∏èüóíÔ∏è

Getting structured  LLM output is hard! Part 3 of our initiative to improve @LangChainAI docs covers this w/ functions and parsers (see @GoogleColab ntbk). Thanks to @fpingham for improving the docs on this:

https://t.co/coFZNhy6be 

![](https://pbs.twimg.com/media/F2yL-yzaIAAY4Ep.jpg)  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883380631932928))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 1/ Getting LLMs to produce structured (e.g., JSON) output is challenge, often requiring tedious prompt eng:
https://t.co/qFQAhdcrzs  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883382578130944))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 2/ Functions (e.g., using OpenAI models) have been a great way to tackle this problem, as shown by the work of @jxnlco and others. LLM calls a function and returns output that follows a specified schema.
https://t.co/Lxa53vIgh5  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883383983210496))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 3/ @LangChainAI support function calling for extraction, requiring only specification of the desired output schema (e.g., JSON, Pydantic class, etc). 

![](https://pbs.twimg.com/media/F2yMyJ-bMAAYqnS.jpg) 

![](https://pbs.twimg.com/media/F2yNG4YaoAEDLv3.jpg)  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883385241579520))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 4/ We can look under the hood using LangSmith trace, seeing that the prompt instructs the LLM (in this case OpenAI) to call "information_extraction" (function defined) here: 
https://t.co/jkOaS1vmGq
Trace: 
https://t.co/gf6DW3r8RM 

![](https://pbs.twimg.com/media/F2yNSorbUAAnC3T.jpg)  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883386688581632))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 5/ The docs provide some examples for using functions, which can shine in cases where we don't specifically a priori all the fields we want to extract. Whereas a parser requires enumeration of each attribute, functions work w/ fields such as "give me extra information": 

![](https://pbs.twimg.com/media/F2yOUJ2bAAAILC_.jpg)  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883388362067968))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 6/ The docs also cover parsers, which are useful esp for LLMs that don't yet support function calling. We can use a LangSmith trace to see that parsers use few shot prompting under the hood:
https://t.co/67CR75nmNs 

![](https://pbs.twimg.com/media/F2yO_e4bcAAY-tg.jpg)  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883389976854528))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 7/ It's worth noting that more LLMs are getting support for function calling (e.g., @AnthropicAI) :
https://t.co/ttMBY8DLZL

Llama2 has been fine-tuned to support it as well:
https://t.co/ISAY1MsnF8  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883391985917952))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 8/ For more in-depth, see past webinars on parsing, extraction, and function calling w/ @GregKamradt, @jerwelborn, @veryboldbagel, @fpingham, @jxnlco 

https://t.co/up5sVML7nd
https://t.co/uRbFgMnxW1  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883393512738816))
** #+END_QUOTE
** üìå
** #+BEGIN_QUOTE
** 9/ And for more on the community initiative to improve the docs, see Part 2 on summarization: 

https://t.co/RDjrK3JIE0  ([View Tweet](https://twitter.com/RLanceMartin/status/1687883827195396096))
** #+END_QUOTE