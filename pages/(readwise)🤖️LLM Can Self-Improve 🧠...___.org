:PROPERTIES:
:title: (readwise)🤖️LLM Can Self-Improve 🧠...
:END:

:PROPERTIES:
:author: [[jkronand on Twitter]]
:full-title: "🤖️LLM Can Self-Improve 🧠..."
:category: [[tweets]]
:url: https://twitter.com/jkronand/status/1621744876298833920
:image-url: https://pbs.twimg.com/profile_images/1635756469986689024/lPOWrGg5.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-05]]
** 📌
#+BEGIN_QUOTE
🤖️LLM can self-improve 🧠

1) Self-consistency boosts reasoning skills by sampling multiple paths & finding the most consistent answer

But more samples = more comp. requirements. 💻

2)  but we can train better LLM with self-generated solutions from 1)

https://t.co/kLfyCuc0uL 
#+END_QUOTE
    date:: [[2023-02-05]]
*** from _🤖️LLM Can Self-Improve 🧠..._ by @jkronand on Twitter
*** [View Tweet](https://twitter.com/jkronand/status/1621744876298833920)
** 📌
#+BEGIN_QUOTE
The naive approach requires:

\- dataset of unlabeled questions
- few-shot chain of thought prompts to generate the "pseudo" labels for the self-train pass

Authors show one can also extend a dataset with synthetic questions, to get more pseudo labels, and improve results further 
#+END_QUOTE
    date:: [[2023-02-05]]
*** from _🤖️LLM Can Self-Improve 🧠..._ by @jkronand on Twitter
*** [View Tweet](https://twitter.com/jkronand/status/1621745961444986880)