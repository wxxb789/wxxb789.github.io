:PROPERTIES:
:title: readwise/The Transformer Architec...
:END:


* metadata
:PROPERTIES:
:author: [[geisler_si on Twitter]]
:full-title: "The Transformer Architec..."
:category: [[tweets]]
:url: https://twitter.com/geisler_si/status/1676259104673677312
:image-url: https://pbs.twimg.com/profile_images/1329198640557150209/uqBodXS3.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-20]]
** ðŸ“Œ
#+BEGIN_QUOTE
The transformer architecture powers recent AI tools like #ChatGPT or #GoogleBard.

In our @DeepMind #ICML2023 paper Transformers Meet Directed Graphs, we generalize transformers to more general inputs, namely directed graphs.

Hereâ€™s how we did it. ðŸ§µ https://t.co/2wpv4n0uPN 

![](https://pbs.twimg.com/media/F0NEJasXsAABPk5.jpg) 
#+END_QUOTE
    date:: [[2023-07-05]]
*** from _The Transformer Architec..._ by @geisler_si on Twitter
*** [[https://twitter.com/geisler_si/status/1676259104673677312][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Transformers on directed graphs are surprisingly underexplored, despite the importance of directed graphs for inputs like source code or logical circuits. Leveraging this input modality, we outperform the prior state of the art on the Open Graph Benchmark Code2 dataset by 15%. 
#+END_QUOTE
    date:: [[2023-07-05]]
*** from _The Transformer Architec..._ by @geisler_si on Twitter
*** [[https://twitter.com/geisler_si/status/1676259107177656320][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Directed graphs can also help to model symmetries in the input domain. For example, with our directed graph construction, our model for source code becomes invariant to certain meaningless perturbations of the inputs, while previous sequence or graph-based models were not. 

![](https://pbs.twimg.com/media/F0NDlSEXsAAn9eJ.jpg) 
#+END_QUOTE
    date:: [[2023-07-05]]
*** from _The Transformer Architec..._ by @geisler_si on Twitter
*** [[https://twitter.com/geisler_si/status/1676259109031534594][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
The backbone of generalizing transformers to directed graphs are our two principled positional encodings for directed graphs. One is based on spectral graph theory using the Magnetic Laplacian, and the other uses random walks. 

![](https://pbs.twimg.com/media/F0NDodwXsAEYmpR.png) 
#+END_QUOTE
    date:: [[2023-07-05]]
*** from _The Transformer Architec..._ by @geisler_si on Twitter
*** [[https://twitter.com/geisler_si/status/1676259111866859523][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
To further the understanding of differences and commonalities of prior positional encodings, we, e.g., make the connection between sinusoidal positional encodings and the eigenvectors of the Laplacian explicit and show how our positional encodings are superior. 
#+END_QUOTE
    date:: [[2023-07-05]]
*** from _The Transformer Architec..._ by @geisler_si on Twitter
*** [[https://twitter.com/geisler_si/status/1676259114261913601][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
You can find a short video presentation at https://t.co/S7iV1t3tho

Moreover, to facilitate reproducibility and adoption to further applications, we open-sourced the code under https://t.co/53WEbXvqBD and provide further resources under https://t.co/uhH6J9R1LT 
#+END_QUOTE
    date:: [[2023-07-05]]
*** from _The Transformer Architec..._ by @geisler_si on Twitter
*** [[https://twitter.com/geisler_si/status/1676259116140863490][View Tweet]]
** ðŸ“Œ
#+BEGIN_QUOTE
Joint work with my great collaborators @liyuajia @DJ_Mankowitz @TaylanCemgilML @guennemann @CauseMean 
#+END_QUOTE
    date:: [[2023-07-05]]
*** from _The Transformer Architec..._ by @geisler_si on Twitter
*** [[https://twitter.com/geisler_si/status/1676259118221258754][View Tweet]]