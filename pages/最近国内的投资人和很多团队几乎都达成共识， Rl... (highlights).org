:PROPERTIES:
:title: 最近国内的投资人和很多团队几乎都达成共识， Rl... (highlights)
:END:

:PROPERTIES:
:author: [[oran_ge on Twitter]]
:full-title: "最近国内的投资人和很多团队几乎都达成共识， Rl..."
:category: [[tweets]]
:url: https://twitter.com/oran_ge/status/1681793774685659136
:END:

* Highlights first synced by [[Readwise]] [[2023-07-20]]
** 📌
** #+BEGIN_QUOTE
** 最近国内的投资人和很多团队几乎都达成共识， RLHF 不重要，SFT 就够了。
现在 Llama2 的论文说 RLHF 非常非常重要。
Llama2 是第三个用 RLHF 比较成功，并把能力推到 Chatgpt 3.5 水平的模型。
这是用实力打脸啊。
期待看到下周的「新共识」了。
😄  ([View Tweet](https://twitter.com/oran_ge/status/1681793774685659136))
** #+END_QUOTE
** 📌
** #+BEGIN_QUOTE
** We note that reward model accuracy is one of the most important proxies for the final performance of Llama 2-Chat.
我们注意到，奖励模型的准确性是Llama 2-Chat最终表现中最重要的指标之一。
（做好 RLHF 先要做好 RM  ([View Tweet](https://twitter.com/oran_ge/status/1681807170407653377))
** #+END_QUOTE
** 📌
** #+BEGIN_QUOTE
** 问题在于，开源模型很多，但没有任何人（包括Meta）开源一个好用的奖励模型，这也属于核心技术了。  ([View Tweet](https://twitter.com/oran_ge/status/1681807612244033542))
** #+END_QUOTE
** 📌
** #+BEGIN_QUOTE
** 如果你有了一个准确的RM，就可以通过模型输出让RM打分，科学合理自动化地判断你 RLHF 是不是在提升模型效果。 

![](https://pbs.twimg.com/media/F1b7ghHaEAAtX_W.jpg)  ([View Tweet](https://twitter.com/oran_ge/status/1681808313506484224))
** #+END_QUOTE
** 📌
** #+BEGIN_QUOTE
** 标注员的上限即是模型的上限：
我们的研究结果强调了RLHF成功的关键因素在于它在注释过程中促进人类和LLM之间的协同作用。
但即使有熟练的注释员，每个人的写作风格也存在显著的差异，模型会学到不好的风格。
此外，模型的性能受到最熟练注释员的写作能力的限制。  ([View Tweet](https://twitter.com/oran_ge/status/1681811225724997632))
** #+END_QUOTE