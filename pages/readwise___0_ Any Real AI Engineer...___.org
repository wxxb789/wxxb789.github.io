:PROPERTIES:
:title: readwise/0_ Any Real AI Engineer...
:END:

:PROPERTIES:
:author: [[jxnlco on Twitter]]
:full-title: "0/ Any Real AI Engineer..."
:category: [[tweets]]
:url: https://twitter.com/jxnlco/status/1681328216361910279
:image-url: https://pbs.twimg.com/profile_images/1682267111555571714/VDORoUy_.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-18]]
** üìå
#+BEGIN_QUOTE
0/ Any real AI engineer knows that streaming REALLY improves the UX. 

Today, I'm landing a change that defines a reliable way to stream out multiple @pydantic objects from @OpenAI . 

Take a look, by the end, you'll know how to do streaming extraction and why it matters. 

![](https://pbs.twimg.com/media/F1VDavQWwAEroWm.jpg) 
#+END_QUOTE
    date:: [[2023-07-19]]
*** from _0/ Any Real AI Engineer..._ by @jxnlco on Twitter
*** [View Tweet](https://twitter.com/jxnlco/status/1681328216361910279)
** üìå
#+BEGIN_QUOTE
1/ Streaming is critical when building applications where the UI is generated by the AI.

Notice in the screenshot that the first item was returned in 560ms but the last one in almost 2000ms! a 4x difference in time to first content

How do we do this? 

![](https://pbs.twimg.com/media/F1VGYvhWwAIa5IA.jpg) 
#+END_QUOTE
    date:: [[2023-07-19]]
*** from _0/ Any Real AI Engineer..._ by @jxnlco on Twitter
*** [View Tweet](https://twitter.com/jxnlco/status/1681328221093089282)
** üìå
#+BEGIN_QUOTE
2/ By using `pip install openai_function_call` we can do the following:

1) Use MultiTask to dynamically make a schema based on a `cls` that we defined
2) Set stream=True to unlock the latency win
3) `from_streaming_response` allows us to parse the completion into generator[cls]. 

![](https://pbs.twimg.com/media/F1VEK3MWwAEmEMj.jpg) 
#+END_QUOTE
    date:: [[2023-07-19]]
*** from _0/ Any Real AI Engineer..._ by @jxnlco on Twitter
*** [View Tweet](https://twitter.com/jxnlco/status/1681328225463549958)
** üìå
#+BEGIN_QUOTE
3/ Why? 

GPT-4 can be pretty slow, but often needed for complex extraction but you don't want users to wait for data to render until the entire payload is generated... Lame

As the schema gets complex or the number of items increases non streaming latency only goes up! 
#+END_QUOTE
    date:: [[2023-07-19]]
*** from _0/ Any Real AI Engineer..._ by @jxnlco on Twitter
*** [View Tweet](https://twitter.com/jxnlco/status/1681328230056198146)
** üìå
#+BEGIN_QUOTE
4/ However, for streaming, the time to first content is fixed! 

We already know know a few 100ms can be a revenue impact on a landing page so don't let it be worse than it needs to be for a production app

If you want to learn more checkout:
https://t.co/SQRvGF3Mfj 
#+END_QUOTE
    date:: [[2023-07-19]]
*** from _0/ Any Real AI Engineer..._ by @jxnlco on Twitter
*** [View Tweet](https://twitter.com/jxnlco/status/1681328233407447040)
** üìå
#+BEGIN_QUOTE
You can also give the repo a look and don't forget to give it a ‚≠êÔ∏è 

If you're interesting in try things out hit me up for some prompting tips

https://t.co/2Ro9PDe6f8 
#+END_QUOTE
    date:: [[2023-07-19]]
*** from _0/ Any Real AI Engineer..._ by @jxnlco on Twitter
*** [View Tweet](https://twitter.com/jxnlco/status/1681328236758724610)
** üìå
#+BEGIN_QUOTE
If you like the content consider following me @jxnlco to follower this journey. 

I‚Äôm encapsulating everything I‚Äôm learning from my consulting business into this library. 

Every line of code in this repo will be run in prod. 
#+END_QUOTE
    date:: [[2023-07-19]]
*** from _0/ Any Real AI Engineer..._ by @jxnlco on Twitter
*** [View Tweet](https://twitter.com/jxnlco/status/1681359870786666508)