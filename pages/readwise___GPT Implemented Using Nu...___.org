:PROPERTIES:
:title: readwise/GPT Implemented Using Nu...
:END:


* metadata
:PROPERTIES:
:author: [[akshay_pachaar on Twitter]]
:full-title: "GPT Implemented Using Nu..."
:category: [[tweets]]
:url: https://twitter.com/akshay_pachaar/status/1627652574315954180
:image-url: https://pbs.twimg.com/profile_images/1578327351544360960/YFpWSWIX.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
GPT implemented using NumPy in like 60 lines of code! ğŸ”¥

Let me break it down for to you in this thread: ğŸ§µğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVXg7aIAcm8rL.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
1ï¸âƒ£ gelu

Applies Gaussian Error Linear Units activation function to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVZw9akAA5Lbx.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
2ï¸âƒ£ softmax

Applies softmax function to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVaclaAAMPUHh.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
3ï¸âƒ£ layer_norm

Applies layer normalization to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVbIgaEAE2yqi.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
4ï¸âƒ£ linear

Performs a linear op on the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVbxKaYAALK-O.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
5ï¸âƒ£ ffn

Passes the input array to a feedforward neural network.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVcV0agAUYV_x.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
6ï¸âƒ£ attention

Applies attention mechanism to the input arrays.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVdFiaEAIl6z5.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
7ï¸âƒ£ mha

Applies multi-head attention to the input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVdrqaAAEYbsR.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
8ï¸âƒ£ transformer_block

A transformer block that applies multi-head attention and ffn to an input array.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVeUvakAAcd6d.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
9ï¸âƒ£ gpt

A transformer network composed of several transformer blocks.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVfEEaYAAIoNK.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
ğŸ”Ÿ generate

Generate new tokens given an initial sequence of tokens and a set of parameters.

Check this out ğŸ‘‡ 

![](https://pbs.twimg.com/media/FpaVf0DaYAkffDl.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
Credits: jaymody (GitHub)

Here's the repo â¬‡ï¸
https://t.co/0srJ2NFDKa 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-21]]
#+BEGIN_QUOTE
That's a wrap!

Everyday, I share tutorials around Data Science & Machine Learning.

Find me â†’ @akshay_pachaar âœ”ï¸

Like/RT the tweet below to support my work! ğŸ™ https://t.co/aNPzY2FqWY 
#+END_QUOTE\