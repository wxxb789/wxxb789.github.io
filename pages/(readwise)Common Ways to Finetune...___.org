:PROPERTIES:
:title: (readwise)Common Ways to Finetune...
:END:

:PROPERTIES:
:author: [[rasbt on Twitter]]
:full-title: "Common Ways to Finetune..."
:category: [[tweets]]
:url: https://twitter.com/rasbt/status/1658111264671911938
:image-url: https://pbs.twimg.com/profile_images/1661187442043486209/a3E4t1eV.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-05]]
** ðŸ“Œ
#+BEGIN_QUOTE
Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://t.co/MZVdpm09Ex). 

Small plot twist via https://t.co/UtPqJ2DN5q

1/3 

![](https://pbs.twimg.com/media/FwLJ65nXsAE12kG.jpg) 
#+END_QUOTE
    date:: [[2023-05-15]]
*** from _Common Ways to Finetune..._ by @rasbt on Twitter
*** [View Tweet](https://twitter.com/rasbt/status/1658111264671911938)
** ðŸ“Œ
#+BEGIN_QUOTE
It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3 

![](https://pbs.twimg.com/media/FwLKwqfWIAAdtnm.jpg) 
#+END_QUOTE
    date:: [[2023-05-15]]
*** from _Common Ways to Finetune..._ by @rasbt on Twitter
*** [View Tweet](https://twitter.com/rasbt/status/1658111267054157825)
** ðŸ“Œ
#+BEGIN_QUOTE
A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://t.co/UtPqJ2DN5q 

![](https://pbs.twimg.com/media/FwLLGSHWwAMjtOI.jpg) 
#+END_QUOTE
    date:: [[2023-05-15]]
*** from _Common Ways to Finetune..._ by @rasbt on Twitter
*** [View Tweet](https://twitter.com/rasbt/status/1658111269973393418)