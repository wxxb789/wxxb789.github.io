:PROPERTIES:
:title: Recent Transformer LMs A... (highlights)
:author: [[albertobietti on Twitter]]
:full-title: "Recent Transformer LMs A..."
:category: [[tweets]]
:url: https://twitter.com/albertobietti/status/1664708139831250962
:END:

* Highlights first synced by [[Readwise]] [[2023-06-04]]
** Recent Transformer LMs are very good at using their context for predicting new tokens.

How does this capability arise during training?

We study this in our paper "Birth of a Transformer: A Memory Viewpoint" üê£

https://t.co/IhcFJUu7Jg 

![](https://pbs.twimg.com/media/Fxo63lsX0A89uQy.png) ([View Tweet](https://twitter.com/albertobietti/status/1664708139831250962))
** We consider a synthetic data model where next tokens follow either global or contextual bigrams.
Empirically, two-layer transformers quickly learn global bigrams, and later develop an induction head (@ch402 @nelhage @catherineols @NeelNanda5) for in-context prediction. 

![](https://pbs.twimg.com/media/Fxo8BffX0Bs4pJ1.jpg) ([View Tweet](https://twitter.com/albertobietti/status/1664709185131499542))
** @ch402 @nelhage @catherineols @NeelNanda5 We present a useful viewpoint for understanding model weights in a transformer as associative memories of high-dim embeddings. The induction head mechanism can be obtained with the following outer-product matrices as "memories", and all others fixed at random initialization: 

![](https://pbs.twimg.com/media/Fxo8Xf6X0AE31DR.png) ([View Tweet](https://twitter.com/albertobietti/status/1664709423556710431))
** @ch402 @nelhage @catherineols @NeelNanda5 Training only those layers successfully recovers the desired associative-memory behaviors, and we can show theoretically how this may be achieved with gradient steps on the population loss, in a top-down manner. 

![](https://pbs.twimg.com/media/Fxo8iHIX0BQWigt.jpg) ([View Tweet](https://twitter.com/albertobietti/status/1664709672631259185))
** @ch402 @nelhage @catherineols @NeelNanda5 Joint work w/ @CabannesVivien @D_Bouchacourt @hjegou and Leon Bottou ([View Tweet](https://twitter.com/albertobietti/status/1664710133388132352))