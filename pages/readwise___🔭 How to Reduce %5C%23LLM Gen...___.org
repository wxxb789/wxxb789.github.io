:PROPERTIES:
:title: readwise/ğŸ”­ How to Reduce \#LLM Gen...
:END:


* metadata
:PROPERTIES:
:author: [[ShayneRedford on Twitter]]
:full-title: "ğŸ”­ How to Reduce \#LLM Gen..."
:category: [[tweets]]
:url: https://twitter.com/ShayneRedford/status/1628068629983150080
:image-url: https://pbs.twimg.com/profile_images/1668811167945441280/3rbesYxR.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-22]]
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸ”­ How to reduce #LLM generation toxicity/bias?

I'm surprised this finding hasn't received any attention:

Instruction Tuning (e.g. Flan, T0) reduces toxic generations A LOT âœ¨ w/o any Human Feedback âœ¨.

â¡ï¸ I.e. #ChatGPT-esque Human values alignment w/o human feedback.

1/ 

![](https://pbs.twimg.com/media/FpgPyQLaMAAu79D.jpg) 

![](https://pbs.twimg.com/media/FpgPyhWaEAEfsJe.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
Buried in the Scaling Flan Appendix C:

1âƒ£ Generating toxic responses to non-toxic prompts (ğŸ”» 44% â¡ï¸ 18%)
2âƒ£ Generating toxic responses to prompts on identity groups (ğŸ”»ğŸ”»)
3âƒ£ Ability to detect toxic content 0-shot (ğŸ”º14%+)

ğŸ“œ: https://t.co/lvYsx8kPfF

2/ 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸŒŸ Finding 1âƒ£ ğŸŒŸ

RealToxicityPrompts measures how often toxic / non-toxic prompts trigger biased/toxic/profane responses (rated by Perspective API).

RTP ğŸ“œ: https://t.co/UcGsE9d8d0, @samgehman @ssgrn et al

Figures: Flan-PaLM's responses eval less toxic than PaLM's

3/ 

![](https://pbs.twimg.com/media/FpgPzE8agAAwcBV.png) 

![](https://pbs.twimg.com/media/FpgPzWVaMAAV0ag.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸŒŸ Finding 2âƒ£ ğŸŒŸ

Responses to prompts for identity groups are systematically less toxic for Flan-PaLM than PaLM.

Figures: Religion/Gender/Ethnicity toxicities lower (L), though tail distribution toxicities aren't solved (R).

4/ 

![](https://pbs.twimg.com/media/FpgPz5sagAIiu7Z.jpg) 

![](https://pbs.twimg.com/media/FpgP0QJaMAEQ8MP.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸŒŸ Finding 3âƒ£ ğŸŒŸ

LLMs' ability to identify whether text is toxic or not, using Civil Comments, is significantly improved (perhaps less surprising).

Civil Comments ğŸ“œ: https://t.co/IjLhkDqOWI

Table: Flan-PaLM > PaLM by ~13%+ for 0-shot, and ~5%+ for 10-shot.

5/ 

![](https://pbs.twimg.com/media/FpgP0x6aAAAhKyC.png) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
Why is this important?

New work shows larger models can harbor more bias (ğŸ“œ: https://t.co/TTKQC1MQot Ganguli, @AmandaAskell, @nschiefer et al.)

And larger models may hallucinate non-factual info more (https://t.co/cs8z1ka8Sz)

6/ 

![](https://pbs.twimg.com/media/FpgP1OwaYAAyb-E.jpg) 

![](https://pbs.twimg.com/media/FpgP1hNaAAYiH2b.jpg) 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸŒŸ Take-aways ğŸŒŸ

Much of the discussion on "alignment to human values" has centered on collecting human feedback signals to model responses.

But the Flan Collection is simply NLP tasks framed as instructions...

7/ 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸŒŸ Take-aways ğŸŒŸ

This isn't to say human feedback doesn't provide stronger benefits, but significant toxicity reduction (or better #AISafety) may be achievable with the tools we already have (simple instruction tuning), even without new large-scale collection efforts.

8/ 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸŒŸ Take-aways ğŸŒŸ

TLDR: Care about bias/toxicity? Use your favorite instruction-tuned model.

(Like Flan-T5: https://t.co/p7gVis9lHM!)

9/ 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
ğŸŒŸ Limitations ğŸŒŸ

â¡ï¸ Toxicity evals are imperfect and not comprehensive!
â¡ï¸ These evals are only English and western-centric
â¡ï¸ Review the paper for important details

10/ 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
Extra credit goes to Kevin Robinson, @Hou_Le, @m_pellat, Dasha Valter, @acastroros who ran these evals.

11/11 
#+END_QUOTE\
** ğŸ“Œ [[2023-02-22]]
#+BEGIN_QUOTE
And thank you to @_jasonwei and @YiTayML for feedback on this thread! 
#+END_QUOTE\