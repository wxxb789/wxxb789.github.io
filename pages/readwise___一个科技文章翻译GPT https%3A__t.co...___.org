:PROPERTIES:
:title: readwise/一个科技文章翻译GPT https:__t.co...
:END:


* metadata
:PROPERTIES:
:author: [[dotey on Twitter]]
:full-title: "一个科技文章翻译GPT https://t.co..."
:category: [[tweets]]
:url: https://twitter.com/dotey/status/1748814824119644272
:image-url: https://pbs.twimg.com/profile_images/561086911561736192/6_g58vEs.jpeg
:END:

* Highlights first synced by [[Readwise]] [[2024-01-21]]
** 📌 [[2024-01-21]]
#+BEGIN_QUOTE
一个科技文章翻译GPT https://t.co/WDvGUzZqFS 的经典案例：

### 直译
对于大语言模型(LLM)来说，理想的训练数据并不是你写下的内容。而是你内心思考的完整序列，以及在写作过程中的所有个别编辑。
但你只能用现有的数据来应对。

停一下，这和 Neuralink 无关，哈哈。
无论如何，这只是启蒙的第一阶段。
启蒙的第二阶段是，对于大语言模型(LLM)来说，理想的训练数据根本就不是训练数据。
而是你从阅读它的人那里得到的赞许。
但你只能用现有的数据来应对。

***

### 问题
1. 第一段中，“对于大语言模型(LLM)来说，理想的训练数据并不是你写下的内容。”这句话在中文中读起来略显生硬，需要调整语序和用词使其更加符合中文习惯。
2. “而是你内心思考的完整序列，以及在写作过程中的所有个别编辑。”这句话的表达略显晦涩，需要更加通俗易懂的表述。
3. “但你只能用现有的数据来应对。”这句话的语气和语境转换较生硬，需要更加流畅的衔接。
4. “停一下，这和 Neuralink 无关，哈哈。”这句话在中文中的直接翻译感觉突兀，需要调整语气和上下文的衔接。
5. “启蒙的第二阶段是，对于大语言模型(LLM)来说，理想的训练数据根本就不是训练数据。”这句话的结构复杂且难以理解，需要更简洁明了的表述。
6. “而是你从阅读它的人那里得到的赞许。”这句话的表达不够直观，需要更加具体和形象化。
7. 整体段落需要更好地连接和过渡，以提高整体的可读性。

***

### 意译
对于大语言模型（LLM）而言，最理想的训练数据并不仅仅是你笔下的文字。它其实包括了你思考过程中的每一个细节，以及写作时所做的每一次修改。
不过，我们只能尽力利用现有的资源。

别误会，这跟 Neuralink 完全没关系，哈哈。
话说回来，这只是认识到这一点的第一步。
第二步则是意识到，对大语言模型（LLM）来说，最理想的“训练数据”其实根本不是数据。
而是读者对你作品的点赞和认可。
但最终，我们还是只能利用手头上的资源。

https://t.co/o8gklWjaRX 
#+END_QUOTE\