:PROPERTIES:
:title: readwise/Meta 发布了一种名为Branch-Train...
:END:


* metadata
:PROPERTIES:
:author: [[op7418 on Twitter]]
:full-title: "Meta 发布了一种名为Branch-Train..."
:category: [[tweets]]
:url: https://twitter.com/op7418/status/1767806016828432807
:image-url: https://pbs.twimg.com/profile_images/1636981205504786434/xDl77JIw.jpg
:END:

* Highlights first synced by [[Readwise]] [[2024-03-19]]
** 📌 [[2024-03-14]]
#+BEGIN_QUOTE
Meta 发布了一种名为Branch-Train-MiX (BTX) 的方法，旨在提高大型语言模型（LLMs）在多个专业领域（如编程、数学推理和世界知识）的能力。

BTX方法从一个种子模型开始，通过并行且高效的方式分支训练专家模型，减少了通信成本。

这些专家模型在训练后，其前馈参数被整合到混合专家（Mixture-of-Expert, MoE）层中，并进行平均参数的MoE微调，以学习在token级别上的路由。

BTX方法结合了Branch-Train-Merge和Mixture-of-Experts两种方法的优点，同时减少了它们的不足。与Branch-Train-Merge相比，BTX最终模型是一个统一的神经网络，可以进行进一步的监督微调（SFT）或人类反馈强化学习（RLHF）微调。与纯MoE训练相比，BTX在计算效率上更高，训练吞吐量更大，且在不同领域的任务上表现更平衡。

实验使用了Llama-2 7B模型作为种子模型，并在数学、编程和维基百科等不同数据子集上训练专家LLMs。通过将原始Llama-2 7B权重作为第四个专家加入，研究者们对合并后的MoE模型进行了相对较短的微调。

结果表明，BTX模型在多个领域的任务上相比种子模型有显著提升，尤其是在数学和编程相关任务上，同时保留了在原始能力上的表现，避免了灾难性遗忘。BTX在所有任务上都优于BTM，展示了通过MoE微调学习路由的好处。与稀疏上循环（sparse upcycling）等纯MoE训练相比，BTX在计算效率上更优，训练吞吐量更高，且在不同领域的任务上表现更平衡。

论文地址：https://t.co/2moJGAotCe

![](https://pbs.twimg.com/media/GIiCA2fawAADCcc.jpg) 
#+END_QUOTE\