:PROPERTIES:
:title: 卡梅隆博士的arXiv论文综述研究越来越接近LL... (highlights)
:END:

:PROPERTIES:
:author: [[TaNGSoFT on Twitter]]
:full-title: "卡梅隆博士的arXiv论文综述研究越来越接近LL..."
:category: [[tweets]]
:url: https://twitter.com/TaNGSoFT/status/1698800631501500793
:END:

* Highlights first synced by [[Readwise]] [[2023-09-05]]
** 📌
** #+BEGIN_QUOTE
** 卡梅隆博士的arXiv论文综述研究越来越接近LLM本质问题思考了：
                    ——————————

下一个令牌预测是因果语言模型的主力。尽管最近取得了进步，但LLM的能力在很大程度上归因于下一个Token预测。为了更好地理解这个概念，让我们研究一下LLM的下一个令牌Token预测的实现...

背景：下一个令牌预测是因果语言模型最常用的训练前目标。它以文本序列作为输入，并训练基础模型来预测此序列中每个令牌的下一个令牌。由于基础真相下一个令牌已经存在于基础文本中（即“标签”是隐含的），这是一个自我监督的培训目标。我们只需从互联网下载大量文本数据，就可以为下一次令牌预测预训练构建一个数据集。

构建输入并使用我们的模型：在预训练期间，我们的LLM将一个序列（或迷你批次中的多个序列）作为输入。我们可以通过tokenizer传递此原始文本，将每个序列分解为离散的令牌。然后，我们嵌入这些令牌（包括位置嵌入），并将生成的向量序列传递到仅解码器decoder-only的变压器Transformer中，该变压器通过几个前馈和因果自关注self-attention层的序列为每个令牌生成输出向量。

预测下一个令牌：对于我们迷你批次的每个序列中的每个令牌，仅解码器的变压器输出一个向量。我们不仅在每个序列中的最后一个令牌上执行下一个令牌预测，而且在每个序列中的每个令牌上并行执行。由于在变压器中使用因果自我关注，这是可能的。没有token知道那些在它之后的人。为了预测下一个令牌，我们使用线性层将令牌的输出向量投影到令牌词汇表上的概率分布中。这只是一个与我们的词汇量大小相同的向量，我们应用softmax使其成为一个分布。

应用损失：要应用下一个令牌预测损失，我们只需在所有预测的下一个令牌上使用交叉熵损失cross-entropy loss。这种损失鼓励模型最大限度地提高我们输入的每个序列中所有令牌的地面真相下一个令牌的概率。值得注意的是，这与我们用于任何正常分类任务的损失相同！我们只是教模型在每个序列的每个位置中正确分类下一个令牌。

实施：要实现这个培训目标，我们不需要做太多！我们只需要为我们的仅解码器变压器创建一个PyTorch模型，编写数据加载器来获取预训练数据，并创建一个可以应用下一个令牌预测损失的训练循环。在所附图像中，我们检查了Andrej Karpathy在NanoGPT中提供的简单实现。在这里，我们可以看到代码与上面提供的语言模型预培训的讨论完全匹配。唯一需要的更改与分布式训练和自动混合精度（AMP）训练有关，这些训练用于提高LLM训练的效率/快速。  ([View Tweet](https://twitter.com/TaNGSoFT/status/1698800631501500793))
** #+END_QUOTE