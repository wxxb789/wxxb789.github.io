:PROPERTIES:
:title: Common Ways to Finetune... (highlights)
:author: [[rasbt on Twitter]]
:full-title: "Common Ways to Finetune..."
:category: [[tweets]]
:url: https://twitter.com/rasbt/status/1658111264671911938
:END:

* Highlights first synced by [[Readwise]] [[2023-05-15]]
** Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://t.co/MZVdpm09Ex). 

Small plot twist via https://t.co/UtPqJ2DN5q

1/3 

![](https://pbs.twimg.com/media/FwLJ65nXsAE12kG.jpg) ([View Tweet](https://twitter.com/rasbt/status/1658111264671911938))
** It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3 

![](https://pbs.twimg.com/media/FwLKwqfWIAAdtnm.jpg) ([View Tweet](https://twitter.com/rasbt/status/1658111267054157825))
** A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://t.co/UtPqJ2DN5q 

![](https://pbs.twimg.com/media/FwLLGSHWwAMjtOI.jpg) ([View Tweet](https://twitter.com/rasbt/status/1658111269973393418))