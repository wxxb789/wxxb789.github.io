:PROPERTIES:
:title: readwise/Using Top-K Lookup for R...
:END:


* metadata
:PROPERTIES:
:author: [[jerryjliu0 on Twitter]]
:full-title: "Using Top-K Lookup for R..."
:category: [[tweets]]
:url: https://twitter.com/jerryjliu0/status/1681816022557560832
:image-url: https://pbs.twimg.com/profile_images/1283610285031460864/1Q4zYhtb.jpg
:END:

* Highlights first synced by [[Readwise]] [[2023-12-20]]
** üìå
#+BEGIN_QUOTE
Using top-k lookup for retrieval in your RAG system is fast but imprecise ü§î

Using cross-encoding as a reranking step can dramatically speed up LLM inference time AND improve accuracy! ‚ö°Ô∏è

Here's how to implement it in @llama_index üëá: https://t.co/CCfEzs5LOH https://t.co/eii2Dvknv3 

![](https://pbs.twimg.com/media/F1cAb99akAAAQ8I.jpg) 
#+END_QUOTE
    date:: [[2023-07-20]]
*** from _Using Top-K Lookup for R..._ by @jerryjliu0 on Twitter
*** [[https://twitter.com/jerryjliu0/status/1681816022557560832][View Tweet]]
** üìå
#+BEGIN_QUOTE
Most existing retrieval systems rely on "bi-encoding": the query/context are embedded independently, and lookup is done with a simple dot product.

Cross-encoding techniques jointly encode query+context; it's slower but more accurate. Let's use it as a reranking step! üí° 
#+END_QUOTE
    date:: [[2023-07-20]]
*** from _Using Top-K Lookup for R..._ by @jerryjliu0 on Twitter
*** [[https://twitter.com/jerryjliu0/status/1681816027552944128][View Tweet]]
** üìå
#+BEGIN_QUOTE
We can use cross-encoding as a *reranking step* AFTER  existing top-k lookup. This allows you to set a much larger top-k for retrieval when building an LLM QA system.

In @llama_index, if you set top_k=10, by default LLM will take ~30 seconds. 

![](https://pbs.twimg.com/media/F1cBJUYakAANZdw.jpg) 
#+END_QUOTE
    date:: [[2023-07-20]]
*** from _Using Top-K Lookup for R..._ by @jerryjliu0 on Twitter
*** [[https://twitter.com/jerryjliu0/status/1681816031134883840][View Tweet]]
** üìå
#+BEGIN_QUOTE
But with cross-encoding based reranking/pruning, the LLM synthesis takes 4 seconds, and the answer is better.

We use an MSMarco SBERT cross-encoder from @huggingface. 

Huge s/o to @jon_chuang for adding this feature.

Notebook: https://t.co/Pclt2AI0Bg 

![](https://pbs.twimg.com/media/F1cBQwnacAAxt46.jpg) 
#+END_QUOTE
    date:: [[2023-07-20]]
*** from _Using Top-K Lookup for R..._ by @jerryjliu0 on Twitter
*** [[https://twitter.com/jerryjliu0/status/1681816034553237504][View Tweet]]