#+alias: large language model,

* [[nlp history]]
* tech
** [[word2vec]]
** [[attention]]
** [[transformer]]
** [[bert]]
** gpt-3 [[few-shot]] learner
* papers
** Distributed Representations of Words and Phrases and their Compositionality (word2vec). Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean. NeurlPS 2013.
** Neural Machine Translation by Jointly Learning to Align and Translate (Attention). Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. ICLR 2015.
** Neural Machine Translation of Rare Words with Subword Units (BPE). Rico Sennrich, Barry Haddow, Alexandra Birch. ACL 2016.
** Attention Is All You Need (Transformers). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. NeurlPS 2017.
** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. NAACL 2019.
** Language Models are Few-Shot Learners (GPT-3). Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan
NeuriPS 2020.
** Language Models are General-Purpose Interfaces. Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei. arXiv 2022.
* pre-training
** model structure
*** encoder-only
*** decoder-only
*** encoder-decoder
*** [[GAN]]
*** [[GNN]]
*** [[CNN]]
*** [[CNN encoder]]
*** [[CNN encoder-decoder]]
** training objectives
*** reconstruction
*** masked-prediction
*** generative
*** contrastive
** data
*** text
*** video
*** image
*** audio
*** text-audio
*** text-image
*** text-video
* [[self-attention]]
* data is important
** 2023 lifearchitect.ai data shared
** data used by openai
* [[SeIf-supervised Training Objectives for Pre-trained Language ModeI]]
* [[links]]
lifearchitect.ai/models
* tuning and prompting
* model shifting
** specific task -> general purpose
** monolingual -> multilingual/cross-lingual
** single modality -> multi-modality
** single turn -> multi-turn
** supervised training with supervised data sets -> self-supervised + fine tuning with supervised data sets -> self-supervised training + in-context learning
* [[plm and llm]]
* [[pre-trained model]]
* [[tuning technologies]]
* [[prompt/pattern]]
* DONE [[prompt/jailbreak]]
* [[gpt-series model architecture]]
* [[from gpt-3 to chatgpt]]
* [[../assets/image_1679762740404_0.png]]
*